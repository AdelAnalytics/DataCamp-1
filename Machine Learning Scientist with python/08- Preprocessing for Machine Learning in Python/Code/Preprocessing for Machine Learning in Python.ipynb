{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing for Machine Learning in Python\n",
    "#### Course Description\n",
    "This course covers the basics of how and when to perform data preprocessing. This essential step in any machine learning project is when you get your data ready for modeling. Between importing and cleaning your data and fitting your machine learning model is when preprocessing comes into play. You'll learn how to standardize your data so that it's in the right form for your model, create new features to best leverage the information in your dataset, and select the best features to improve your model fit. Finally, you'll have some practice preprocessing by getting a dataset on UFO sightings ready for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction to Data Preprocessing\n",
    "\n",
    "In this chapter you'll learn exactly what it means to preprocess data. You'll take the first steps in any preprocessing journey, including exploring data types and dealing with missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing data - rows\n",
    "Taking a look at the volunteer dataset again, we want to drop rows where the category_desc column values are missing. We're going to do this using boolean indexing, by checking to see if we have any null values, and then filtering the dataset so that we only have rows with those values.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Check how many values are missing in the category_desc column using isnull() and sum().\n",
    "- Subset the volunteer dataset by indexing by where category_desc is notnull(), and store in a new variable called volunteer_subset.\n",
    "- Take a look at the .shape attribute of the new dataset, to verify it worked correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_cols = ['opportunity_id', 'content_id', 'vol_requests', 'event_time', 'title',\n",
    "       'hits', 'summary', 'is_priority', 'category_id', 'category_desc',\n",
    "       'amsl', 'amsl_unit', 'org_title', 'org_content_id', 'addresses_count',\n",
    "       'locality', 'region', 'postalcode', 'primary_loc', 'display_url',\n",
    "       'recurrence_type', 'hours', 'created_date', 'last_modified_date',\n",
    "       'start_date_date', 'end_date_date', 'status', 'Latitude', 'Longitude',\n",
    "       'Community Board', 'Community Council ', 'Census Tract', 'BIN', 'BBL',\n",
    "       'NTA']\n",
    "volunteer = pd.read_csv('data/volunteer_opportunities.csv', \n",
    "                            usecols=vol_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 665 entries, 0 to 664\n",
      "Data columns (total 35 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   opportunity_id      665 non-null    int64  \n",
      " 1   content_id          665 non-null    int64  \n",
      " 2   vol_requests        665 non-null    int64  \n",
      " 3   event_time          665 non-null    int64  \n",
      " 4   title               665 non-null    object \n",
      " 5   hits                665 non-null    int64  \n",
      " 6   summary             665 non-null    object \n",
      " 7   is_priority         62 non-null     object \n",
      " 8   category_id         617 non-null    float64\n",
      " 9   category_desc       617 non-null    object \n",
      " 10  amsl                0 non-null      float64\n",
      " 11  amsl_unit           0 non-null      float64\n",
      " 12  org_title           665 non-null    object \n",
      " 13  org_content_id      665 non-null    int64  \n",
      " 14  addresses_count     665 non-null    int64  \n",
      " 15  locality            595 non-null    object \n",
      " 16  region              665 non-null    object \n",
      " 17  postalcode          659 non-null    float64\n",
      " 18  primary_loc         0 non-null      float64\n",
      " 19  display_url         665 non-null    object \n",
      " 20  recurrence_type     665 non-null    object \n",
      " 21  hours               665 non-null    int64  \n",
      " 22  created_date        665 non-null    object \n",
      " 23  last_modified_date  665 non-null    object \n",
      " 24  start_date_date     665 non-null    object \n",
      " 25  end_date_date       665 non-null    object \n",
      " 26  status              665 non-null    object \n",
      " 27  Latitude            0 non-null      float64\n",
      " 28  Longitude           0 non-null      float64\n",
      " 29  Community Board     0 non-null      float64\n",
      " 30  Community Council   0 non-null      float64\n",
      " 31  Census Tract        0 non-null      float64\n",
      " 32  BIN                 0 non-null      float64\n",
      " 33  BBL                 0 non-null      float64\n",
      " 34  NTA                 0 non-null      float64\n",
      "dtypes: float64(13), int64(8), object(14)\n",
      "memory usage: 182.0+ KB\n"
     ]
    }
   ],
   "source": [
    "volunteer.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n",
      "(617, 35)\n"
     ]
    }
   ],
   "source": [
    "# Check how many values are missing in the category_desc column\n",
    "print(volunteer['category_desc'].isnull().sum())\n",
    "\n",
    "# Subset the volunteer dataset\n",
    "volunteer_subset = volunteer[volunteer['category_desc'].notnull()]\n",
    "\n",
    "# Print out the shape of the subset\n",
    "print(volunteer_subset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting a column type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    737\n",
      "1     22\n",
      "2     62\n",
      "3     14\n",
      "4     31\n",
      "Name: hits, dtype: int64\n",
      "int64\n"
     ]
    }
   ],
   "source": [
    "# Print the head of the hits column\n",
    "print(volunteer[\"hits\"].head())\n",
    "\n",
    "# Convert the hits column to type int\n",
    "volunteer[\"hits\"] = volunteer[\"hits\"].astype('int64')\n",
    "\n",
    "# Look at the dtypes of the dataset\n",
    "print(volunteer.hits.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified sampling\n",
    "We know that the distribution of variables in the category_desc column in the volunteer dataset is uneven. If we wanted to train a model to try to predict category_desc, we would want to train the model on a sample of data that is representative of the entire dataset. Stratified sampling is a way to achieve this.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Create a volunteer_X dataset with all of the columns except category_desc.\n",
    "- Create a volunteer_y training labels dataset.\n",
    "- Split up the volunteer_X dataset using scikit-learn's train_test_split function and passing volunteer_y into the stratify= parameter.\n",
    "- Take a look at the category_desc value counts on the training labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strengthening Communities    230\n",
      "Helping Neighbors in Need     89\n",
      "Education                     69\n",
      "Health                        39\n",
      "Environment                   24\n",
      "Emergency Preparedness        11\n",
      "Name: category_desc, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "volunteer.fillna(123456789, inplace=True)\n",
    "# Create a data with all columns except category_desc\n",
    "volunteer_X = volunteer.drop('category_desc', axis=1)\n",
    "\n",
    "# Create a category_desc labels dataset\n",
    "volunteer_y = volunteer[['category_desc']]\n",
    "\n",
    "# Use stratified sampling to split up the dataset according to the volunteer_y dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(volunteer_X,\n",
    "                                    volunteer_y, stratify = volunteer_y)\n",
    "X_train = X_train.replace(123456789, np.nan) \n",
    "X_test = X_test.replace(123456789, np.nan)\n",
    "y_train = y_train.replace(123456789, np.nan)\n",
    "y_test = y_test.replace(123456789, np.nan)\n",
    "volunteer = volunteer.replace(123456789, np.nan)\n",
    "\n",
    "# Print out the category_desc counts on the training y labels\n",
    "print(y_train['category_desc'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Standardizing Data\n",
    "\n",
    "This chapter is all about standardizing data. Often a model will make some assumptions about the distribution or scale of your features. Standardization is a way to make your data fit these assumptions and improve the algorithm's performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling without normalizing\n",
    "Let's take a look at what might happen to your model's accuracy if you try to model data without doing some sort of standardization first. Here we have a subset of the wine dataset. One of the columns, Proline, has an extremely high variance compared to the other columns. This is an example of where a technique like log normalization would come in handy, which you'll learn about in the next section.\n",
    "\n",
    "The scikit-learn model training process should be familiar to you at this point, so we won't go too in-depth with it. You already have a k-nearest neighbors model available (knn) as well as the X and y sets you need to fit and score on.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Split up the X and y sets into training and test sets using train_test_split().\n",
    "- Use the knn model's fit() method on the X_train data and y_train labels, to fit the model to the data.\n",
    "- Print out the knn model's score() on the X_test data and y_test labels to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = pd.read_csv('data/wine_types.csv')\n",
    "X = wine.drop('Type', axis = 1)\n",
    "y = wine.Type\n",
    "knn  = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7111111111111111\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset and labels into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# Fit the k-nearest neighbors model to the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Score the model on the test data\n",
    "print(knn.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log normalization in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99166.71735542428\n",
      "0.17231366191842018\n"
     ]
    }
   ],
   "source": [
    "# Print out the variance of the Proline column\n",
    "print(wine.Proline.var())\n",
    "\n",
    "# Apply the log normalization function to the Proline column\n",
    "wine['Proline_log'] = np.log(wine.Proline)\n",
    "\n",
    "# Check the variance of the normalized Proline column\n",
    "print(wine.Proline_log.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling data - standardizing columns\n",
    "Since we know that the Ash, Alcalinity of ash, and Magnesium columns in the wine dataset are all on different scales, let's standardize them in a way that allows for use in a linear model.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Import StandardScaler from sklearn.preprocessing.\n",
    "- Create the StandardScaler() method and store in a variable named ss.\n",
    "- Create a subset of the wine DataFrame of the Ash, Alcalinity of ash, and - Magnesium columns, store in a variable named wine_subset.\n",
    "- Apply the ss.fit_transform method to the wine_subset DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the scaler\n",
    "ss = StandardScaler()\n",
    "\n",
    "# Take a subset of the DataFrame you want to scale \n",
    "wine_subset = wine[['Ash', 'Alcalinity of ash',  'Magnesium']]\n",
    "\n",
    "# Apply the scaler to the DataFrame subset\n",
    "wine_subset_scaled = ss.fit_transform(wine_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN on non-scaled data\n",
    "Let's first take a look at the accuracy of a K-nearest neighbors model on the wine dataset without standardizing the data. The knn model as well as the X and y data and labels sets have been created already. Most of this process of creating models in scikit-learn should look familiar to you.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Split the dataset into training and test sets using train_test_split().\n",
    "- Use the knn model's fit() method on the X_train data and y_train labels, to fit the model to the data.\n",
    "- Print out the knn model's score() on the X_test data and y_test labels to evaluate the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7333333333333333\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset and labels into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "\n",
    "# Fit the k-nearest neighbors model to the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Score the model on the test data\n",
    "print(knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN on scaled data\n",
    "The accuracy score on the unscaled wine dataset was decent, but we can likely do better if we scale the dataset. The process is mostly the same as the previous exercise, with the added step of scaling the data. Once again, the knn model as well as the X and y data and labels set have already been created for you.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Create the StandardScaler() method, stored in a variable named ss.\n",
    "- Apply the ss.fit_transform method to the X dataset.\n",
    "- Use the knn model's fit() method on the X_train data and y_train labels, to fit the model to the data.\n",
    "- Print out the knn model's score() on the X_test data and y_test labels to evaluate the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "# Create the scaling method.\n",
    "ss = StandardScaler()\n",
    "\n",
    "# Apply the scaling method to the dataset used for modeling.\n",
    "X_scaled = ss.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y)\n",
    "\n",
    "# Fit the k-nearest neighbors model to the training data.\n",
    "knn.fit(X_train,y_train)\n",
    "\n",
    "# Score the model on the test data.\n",
    "print(knn.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature Engineering\n",
    "\n",
    "In this section you'll learn about feature engineering. You'll explore different ways to create new, more useful, features from the ones already in your dataset. You'll see how to encode, aggregate, and extract information from both numerical and textual features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding categorical variables - binary\n",
    "Take a look at the hiking dataset. There are several columns here that need encoding, one of which is the Accessible column, which needs to be encoded in order to be modeled. Accessible is a binary feature, so it has two values - either Y or N - so it needs to be encoded into 1s and 0s. Use scikit-learn's LabelEncoder method to do that transformation.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Store LabelEncoder() in a variable named enc\n",
    "- Using the encoder's fit_transform() function, encode the hiking dataset's \"Accessible\" column. Call the new column Accessible_enc.\n",
    "- Compare the two columns side-by-side to see the encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiking = pd.read_json('data/hiking.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Accessible_enc Accessible\n",
      "0               1          Y\n",
      "1               0          N\n",
      "2               0          N\n",
      "3               0          N\n",
      "4               0          N\n"
     ]
    }
   ],
   "source": [
    "# Set up the LabelEncoder object\n",
    "enc = LabelEncoder()\n",
    "\n",
    "# Apply the encoding to the \"Accessible\" column\n",
    "hiking['Accessible_enc'] = enc.fit_transform(hiking.Accessible)\n",
    "\n",
    "# Compare the two columns\n",
    "print(hiking[['Accessible_enc', 'Accessible']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding categorical variables - one-hot\n",
    "One of the columns in the volunteer dataset, category_desc, gives category descriptions for the volunteer opportunities listed. Because it is a categorical variable with more than two categories, we need to use one-hot encoding to transform this column numerically. Use Pandas' get_dummies() function to do so.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Call get_dummies() on the volunteer[\"category_desc\"] column to create the encoded columns and assign it to category_enc.\n",
    "- Print out the head() of the category_enc variable to take a look at the encoded columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Education  Emergency Preparedness  Environment  Health  \\\n",
      "0          0                       0            0       0   \n",
      "1          0                       0            0       0   \n",
      "2          0                       0            0       0   \n",
      "3          0                       0            0       0   \n",
      "4          0                       0            1       0   \n",
      "\n",
      "   Helping Neighbors in Need  Strengthening Communities  \n",
      "0                          0                          0  \n",
      "1                          0                          1  \n",
      "2                          0                          1  \n",
      "3                          0                          1  \n",
      "4                          0                          0  \n"
     ]
    }
   ],
   "source": [
    "# Transform the category_desc column\n",
    "category_enc = pd.get_dummies(volunteer.category_desc)\n",
    "\n",
    "# Take a look at the encoded columns\n",
    "print(category_enc.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engineering numerical features - taking an average\n",
    "A good use case for taking an aggregate statistic to create a new feature is to take the mean of columns. Here, you have a DataFrame of running times named running_times_5k. For each name in the dataset, take the mean of their 5 run times.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Create a list of the columns you want to take the average of and store it in a variable named run_columns.\n",
    "- Use apply to take the mean() of the list of columns and remember to set axis=1. Use lambda row: in the apply.\n",
    "- Print out the DataFrame to see the mean column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_dic = {'name': ['Sue', 'Mark', 'Sean', 'Erin', 'Jenny', 'Russell'],\n",
    "             'run1': [20.1, 16.5, 23.5, 21.7, 25.8, 30.9],\n",
    "             'run2': [18.5, 17.1, 25.1, 21.1, 27.1, 29.6],\n",
    "             'run3': [19.6, 16.9, 25.2, 20.9, 26.1, 31.4],\n",
    "             'run4': [20.3, 17.6, 24.6, 22.1, 26.7, 30.4],\n",
    "             'run5': [18.3, 17.3, 23.9, 22.2, 26.9, 29.9]}\n",
    "running_times_5k = pd.DataFrame(running_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      name  run1  run2  run3  run4  run5   mean\n",
      "0      Sue  20.1  18.5  19.6  20.3  18.3  19.36\n",
      "1     Mark  16.5  17.1  16.9  17.6  17.3  17.08\n",
      "2     Sean  23.5  25.1  25.2  24.6  23.9  24.46\n",
      "3     Erin  21.7  21.1  20.9  22.1  22.2  21.60\n",
      "4    Jenny  25.8  27.1  26.1  26.7  26.9  26.52\n",
      "5  Russell  30.9  29.6  31.4  30.4  29.9  30.44\n"
     ]
    }
   ],
   "source": [
    "# Create a list of the columns to average\n",
    "run_columns = [col for col in running_times_5k.columns  if running_times_5k[col].dtype == 'float64']\n",
    "\n",
    "# Use apply to create a mean column\n",
    "running_times_5k[\"mean\"] = running_times_5k.apply(lambda row: row[run_columns].mean(), axis=1)\n",
    "\n",
    "# Take a look at the results\n",
    "print(running_times_5k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engineering numerical features - datetime\n",
    "There are several columns in the volunteer dataset comprised of datetimes. Let's take a look at the start_date_date column and extract just the month to use as a feature for modeling.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Use Pandas to_datetime() function on the volunteer[\"start_date_date\"] column and store it in a new column called start_date_converted.\n",
    "- To retrieve just the month, apply a lambda function to volunteer[\"start_date_converted\"] that grabs the .month attribute from the row. Store this in a new column called start_date_month.\n",
    "- Print the head() of just the start_date_converted and start_date_month columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  start_date_converted  start_date_month\n",
      "0           2011-07-30                 7\n",
      "1           2011-02-01                 2\n",
      "2           2011-01-29                 1\n",
      "3           2011-02-14                 2\n",
      "4           2011-02-05                 2\n"
     ]
    }
   ],
   "source": [
    "# First, convert string column to date column\n",
    "volunteer[\"start_date_converted\"] = pd.to_datetime(volunteer[\"start_date_date\"])\n",
    "\n",
    "# Extract just the month from the converted column\n",
    "volunteer[\"start_date_month\"] = volunteer[\"start_date_converted\"].apply(lambda row: row.month)\n",
    "\n",
    "# Take a look at the converted and new month columns\n",
    "print(volunteer[[\"start_date_converted\", \"start_date_month\"]].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engineering features from strings - extraction\n",
    "The Length column in the hiking dataset is a column of strings, but contained in the column is the mileage for the hike. We're going to extract this mileage using regular expressions, and then use a lambda in Pandas to apply the extraction to the DataFrame.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Create a pattern that will extract numbers and decimals from text, using \\d+ to get numbers and \\. to get decimals, and pass it into re's compile function.\n",
    "- Use re's match function to search the text, passing in the pattern and the length text.\n",
    "- Use the matched mile's group() attribute to extract the matched pattern, making sure to match group 0, and pass it into float.\n",
    "- Apply the return_mileage() function to the hiking[\"Length\"] column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Length  Length_num\n",
      "0   0.8 miles        0.80\n",
      "1    1.0 mile        1.00\n",
      "2  0.75 miles        0.75\n",
      "3   0.5 miles        0.50\n",
      "4   0.5 miles        0.50\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# Write a pattern to extract numbers and decimals\n",
    "def return_mileage(length):\n",
    "    if type(length)!= str:\n",
    "        length = str(length)\n",
    "    pattern = re.compile(r\"\\d+\\.\\d+\")\n",
    "    \n",
    "    # Search the text for matches\n",
    "    mile = re.match(pattern, length)\n",
    "    \n",
    "    # If a value is returned, use group(0) to return the found value\n",
    "    if mile is not None:\n",
    "        return float(mile.group(0))\n",
    "        \n",
    "# Apply the function to the Length column and take a look at both columns\n",
    "hiking[\"Length_num\"] = hiking[\"Length\"].apply(lambda row: return_mileage(row))\n",
    "print(hiking[[\"Length\", \"Length_num\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engineering features from strings - tf/idf\n",
    "Let's transform the volunteer dataset's title column into a text vector, to use in a prediction task in the next exercise.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Store the volunteer[\"title\"] column in a variable named title_text.\n",
    "- Use the tfidf_vec vectorizer's fit_transform() function on title_text to transform the text into a tf-idf vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "volunteer = volunteer[~volunteer.category_desc.isna()]\n",
    "# Take the title text\n",
    "title_text = volunteer[\"title\"]\n",
    "\n",
    "# Create the vectorizer method\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "\n",
    "# Transform the text into tf-idf vectors\n",
    "text_tfidf = tfidf_vec.fit_transform(title_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text classification using tf/idf vectors\n",
    "Now that we've encoded the volunteer dataset's title column into tf/idf vectors, let's use those vectors to try to predict the category_desc column.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Using train_test_split, split the text_tfidf vector, along with your y variable, into training and test sets. Set the stratify parameter equal to y, since the class distribution is uneven. Notice that we have to run the toarray() method on the tf/idf vector, in order to get in it the proper format for scikit-learn.\n",
    "- Use Naive Bayes' fit() method on the X_train and y_train variables.\n",
    "- Print out the score() of the X_test and y_test variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5483870967741935\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset according to the class distribution of category_desc\n",
    "y = volunteer[\"category_desc\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_tfidf.toarray(), y, stratify=y)\n",
    "\n",
    "nb = GaussianNB()\n",
    "# Fit the model to the training data\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Print out the model's accuracy\n",
    "print(nb.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Selecting features for modeling\n",
    "\n",
    "This chapter goes over a few different techniques for selecting the most important features from your dataset. You'll learn how to drop redundant features, work with text vectors, and reduce the number of features in your dataset using principal component analysis (PCA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling without normalizing\n",
    "Let's take a look at what might happen to your model's accuracy if you try to model data without doing some sort of standardization first. Here we have a subset of the wine dataset. One of the columns, Proline, has an extremely high variance compared to the other columns. This is an example of where a technique like log normalization would come in handy, which you'll learn about in the next section.\n",
    "\n",
    "The scikit-learn model training process should be familiar to you at this point, so we won't go too in-depth with it. You already have a k-nearest neighbors model available (knn) as well as the X and y sets you need to fit and score on.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Split up the X and y sets into training and test sets using train_test_split().\n",
    "- Use the knn model's fit() method on the X_train data and y_train labels, to fit the model to the data.\n",
    "- Print out the knn model's score() on the X_test data and y_test labels to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "X = wine.drop('Type', axis = 1)\n",
    "y= wine.Type\n",
    "# Split the dataset and labels into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "# Fit the k-nearest neighbors model to the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Score the model on the test data\n",
    "print(knn.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting relevant features\n",
    "Now let's identify the redundant columns in the volunteer dataset and perform feature selection on the dataset to return a DataFrame of the relevant features.\n",
    "\n",
    "For example, if you explore the volunteer dataset in the console, you'll see three features which are related to location: locality, region, and postalcode. They contain repeated information, so it would make sense to keep only one of the features.\n",
    "\n",
    "There are also features that have gone through the feature engineering process: columns like Education and Emergency Preparedness are a product of encoding the categorical variable category_desc, so category_desc itself is redundant now.\n",
    "\n",
    "Take a moment to examine the features of volunteer in the console, and try to identify the redundant features.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Create a list of redundant column names and store it in the to_drop variable:\n",
    "- Out of all the location-related features, keep only postcode.\n",
    "- Features that have gone through the feature engineering process are redundant as well.\n",
    "- Drop the columns from the dataset using .drop().\n",
    "- Print out the .head() of the DataFrame to see the selected columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "volunteer = pd.read_csv('data/volunteer.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  hits  postalcode  \\\n",
      "0                                       Web designer    22     10010.0   \n",
      "1      Urban Adventures - Ice Skating at Lasker Rink    62     10026.0   \n",
      "2  Fight global hunger and support women farmers ...    14      2114.0   \n",
      "3                                      Stop 'N' Swap    31     10455.0   \n",
      "4                               Queens Stop 'N' Swap   135     11372.0   \n",
      "\n",
      "   vol_requests_lognorm  created_month  Education  Emergency Preparedness  \\\n",
      "0              0.693147              1          0                       0   \n",
      "1              2.995732              1          0                       0   \n",
      "2              6.214608              1          0                       0   \n",
      "3              2.708050              1          0                       0   \n",
      "4              2.708050              1          0                       0   \n",
      "\n",
      "   Environment  Health  Helping Neighbors in Need  Strengthening Communities  \n",
      "0            0       0                          0                          1  \n",
      "1            0       0                          0                          1  \n",
      "2            0       0                          0                          1  \n",
      "3            1       0                          0                          0  \n",
      "4            1       0                          0                          0  \n"
     ]
    }
   ],
   "source": [
    "# Create a list of redundant column names to drop\n",
    "to_drop = [\"category_desc\", \"created_date\", \"locality\", \"region\", \"vol_requests\"]\n",
    "\n",
    "# Drop those columns from the dataset\n",
    "volunteer_subset = volunteer.drop(to_drop, axis=1)\n",
    "\n",
    "# Print out the head of the new dataset\n",
    "print(volunteer_subset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Putting it all together\n",
    "\n",
    "Now that you've learned all about preprocessing you'll try these techniques out on a dataset that records information on UFO sightings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

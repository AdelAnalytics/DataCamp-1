{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to TensorFlow in Python\n",
    "#### Course Description\n",
    "Not long ago, cutting-edge computer vision algorithms couldn’t differentiate between images of cats and dogs. Today, a skilled data scientist equipped with nothing more than a laptop can classify tens of thousands of objects with greater accuracy than the human eye. In this course, you will use TensorFlow 2.1 to develop, train, and make predictions with the models that have powered major advances in recommendation systems, image classification, and FinTech. You will learn both high-level APIs, which will enable you to design and train deep learning models in 15 lines of code, and low-level APIs, which will allow you to move beyond off-the-shelf routines. You will also learn to accurately predict housing prices, credit card borrower defaults, and images of sign language gestures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "constant = tf.constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction to TensorFlow\n",
    "\n",
    "Before you can build advanced models in TensorFlow 2.0, you will first need to understand the basics. In this chapter, you’ll learn how to define constants and variables, perform tensor addition and multiplication, and compute derivatives. Knowledge of linear algebra will be helpful, but not necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining data as constants\n",
    "Throughout this course, we will use tensorflow version 2.1 and will exclusively import the submodules needed to complete each exercise. This will usually be done for you, but you will do it in this exercise by importing constant from tensorflow.\n",
    "\n",
    "After you have imported constant, you will use it to transform a numpy array, credit_numpy, into a tensorflow constant, credit_constant. This array contains feature columns from a dataset on credit card holders and is previewed in the image below. We will return to this dataset in later chapters.\n",
    "\n",
    "Note that tensorflow version 2.0 allows you to use data as either a numpy array or a tensorflow constant object. Using a constant will ensure that any operations performed with that object are done in tensorflow.\n",
    "\n",
    "This image shows four feature columns from a dataset on credit card default: education, marriage, age, and bill amount.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Import the constant submodule from the tensorflow module.\n",
    "- Convert the credit_numpy array into a constant object in tensorflow. Do not set the data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_numpy = pd.read_csv('data/uci_credit_card.csv').iloc[:,:4]\n",
    "credit_numpy = credit_numpy.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The datatype is: <dtype: 'float64'>\n",
      "The shape is: (30000, 4)\n"
     ]
    }
   ],
   "source": [
    "# Convert the credit_numpy array into a tensorflow constant\n",
    "credit_constant = constant(credit_numpy)\n",
    "\n",
    "# Print constant datatype\n",
    "print('The datatype is:', credit_constant.dtype)\n",
    "\n",
    "# Print constant shape\n",
    "print('The shape is:', credit_constant.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining variables\n",
    "Unlike a constant, a variable's value can be modified. This will be quite useful when we want to train a model by updating its parameters. Constants can't be used for this purpose, so variables are the natural choice.\n",
    "\n",
    "Let's try defining and working with a variable. Note that Variable(), which is used to create a variable tensor, has been imported from tensorflow and is available to use in the exercise.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Define a variable, A1, as the 1-dimensional tensor: [1, 2, 3, 4].\n",
    "- Print A1. Do not use the .numpy() method. What did this tell you?\n",
    "- Apply .numpy() to A1 and assign it to B1.\n",
    "- Print B1. What did this tell you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(4,) dtype=int32, numpy=array([1, 2, 3, 4])>\n",
      "[1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "Variable =  tf.Variable\n",
    "# Define the 1-dimensional variable A1\n",
    "A1 = Variable([1, 2, 3, 4])\n",
    "\n",
    "# Print the variable A1\n",
    "print(A1)\n",
    "\n",
    "# Convert A1 to a numpy array and assign it to B1\n",
    "B1 = A1.numpy()\n",
    "\n",
    "# Print B1\n",
    "print(B1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing element-wise multiplication\n",
    "Element-wise multiplication in TensorFlow is performed using two tensors with identical shapes. This is because the operation multiplies elements in corresponding positions in the two tensors. An example of an element-wise multiplication, denoted by the ⊙ symbol, is shown below:\n",
    "\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "1&2 \\\\ 2&1 \n",
    "\\end{bmatrix} \n",
    "\\odot\n",
    "\\begin{bmatrix}\n",
    "3&2 \\\\ 1&5\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "3 & 4 \\\\ 2 & 5\n",
    "\\end{bmatrix} \n",
    "$\n",
    "\n",
    "In this exercise, you will perform element-wise multiplication, paying careful attention to the shape of the tensors you multiply. Note that multiply(), constant(), and ones_like() have been imported for you.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "-  Define the tensors A1 and A23 as constants.\n",
    "-  Set B1 to be a tensor of ones with the same shape as A1.\n",
    "-  Set B23 to be a tensor of ones with the same shape as A23.\n",
    "-  Set C1 and C23 equal to the element-wise products of A1 and B1, and A23 and B23, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C1: [1 2 3 4]\n",
      "C23: [[1 2 3]\n",
      " [1 6 4]]\n"
     ]
    }
   ],
   "source": [
    "# Define tensors A1 and A23 as constants\n",
    "A1 = constant([1, 2, 3, 4])\n",
    "A23 = constant([[1, 2, 3], [1, 6, 4]])\n",
    "\n",
    "# Define B1 and B23 to have the correct shape\n",
    "B1 = tf.ones_like(A1)\n",
    "B23 = tf.ones_like(A23)\n",
    "\n",
    "# Perform element-wise multiplication\n",
    "C1 = tf.multiply(A1,B1)\n",
    "C23 = tf.multiply(A23,B23)\n",
    "\n",
    "# Print the tensors C1 and C23\n",
    "print('C1: {}'.format(C1.numpy()))\n",
    "print('C23: {}'.format(C23.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions with matrix multiplication\n",
    "In later chapters, you will learn to train linear regression models. This process will yield a vector of parameters that can be multiplied by the input data to generate predictions. In this exercise, you will use input data, features, and a target vector, bill, which are taken from a credit card dataset we will use later in the course.\n",
    "\n",
    "<img src=\"im1.JPG\" width=200 />\n",
    "\n",
    "The matrix of input data, features, contains two columns: education level and age. The target vector, bill, is the size of the credit card borrower's bill.\n",
    "\n",
    "Since we have not trained the model, you will enter a guess for the values of the parameter vector, params. You will then use matmul() to perform matrix multiplication of features by params to generate predictions, billpred, which you will compare with bill. Note that we have imported matmul() and constant().\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Define features, params, and bill as constants.\n",
    "- Compute the predicted value vector, billpred, by multiplying the input data, features, by the parameters, params. Use matrix multiplication, rather than the element-wise product.\n",
    "- Define error as the targets, bill, minus the predicted values, billpred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1687]\n",
      " [-3218]\n",
      " [-1933]\n",
      " [57850]]\n"
     ]
    }
   ],
   "source": [
    "# Define features, params, and bill as constants\n",
    "features = constant([[2, 24], [2, 26], [2, 57], [1, 37]])\n",
    "params = constant([[1000], [150]])\n",
    "bill = constant([[3913], [2682], [8617], [64400]])\n",
    "\n",
    "# Compute billpred using features and params\n",
    "billpred = tf.matmul(features,params)\n",
    "\n",
    "# Compute and print the error\n",
    "error = bill - billpred\n",
    "print(error.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping tensors\n",
    "Later in the course, you will classify images of sign language letters using a neural network. In some cases, the network will take 1-dimensional tensors as inputs, but your data will come in the form of images, which will either be either 2- or 3-dimensional tensors, depending on whether they are grayscale or color images.\n",
    "\n",
    "The figure below shows grayscale and color images of the sign language letter A. The two images have been imported for you and converted to the numpy arrays gray_tensor and color_tensor. Reshape these arrays into 1-dimensional vectors using the reshape operation, which has been imported for you from tensorflow. Note that the shape of gray_tensor is 28x28 and the shape of color_tensor is 28x28x3.\n",
    "\n",
    "This figure shows grayscale and color images of the sign language letter \"A\".\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Reshape gray_tensor from a 28x28 matrix into a 784x1 vector named gray_vector.\n",
    "- Reshape color_tensor from a 28x28x3 tensor into a 2352x1 vector named color_vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arrays import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the grayscale image tensor into a vector\n",
    "gray_vector = tf.reshape(gray_tensor, (784, 1))\n",
    "\n",
    "# Reshape the color image tensor into a vector\n",
    "color_vector = tf.reshape(color_tensor, (2352, 1))"
   ]
  },
  {
   "attachments": {
    "gradient_plot.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAEgCAYAAABchszxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing with gradients\n",
    "You are given a loss function, y=x2, which you want to minimize. You can do this by computing the slope using the GradientTape() operation at different values of x. If the slope is positive, you can decrease the loss by lowering x. If it is negative, you can decrease it by increasing x. This is how gradient descent works.\n",
    "\n",
    "<img src=\"gradient_plot.png\" width=300 />\n",
    "\n",
    "\n",
    "The image shows a plot of y equals x squared. It also shows the gradient at x equals -1, x equals 0, and x equals 1.\n",
    "\n",
    "In practice, you will use a high level tensorflow operation to perform gradient descent automatically. In this exercise, however, you will compute the slope at x values of -1, 1, and 0. The following operations are available: GradientTape(), multiply(), and Variable().\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Define x as a variable with the initial value x0.\n",
    "- Set the loss function, y, equal to x multiplied by x. Do not make use of operator overloading.\n",
    "- Set the function to return the gradient of y with respect to x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.0\n",
      "2.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "def compute_gradient(x0):\n",
    "  \t# Define x as a variable with an initial value of x0\n",
    "\tx = tf.Variable(x0)\n",
    "\twith tf.GradientTape() as tape:\n",
    "\t\ttape.watch(x)\n",
    "        # Define y using the multiply operation\n",
    "\t\ty = x*x\n",
    "    # Return the gradient of y with respect to x\n",
    "\treturn tape.gradient(y, x).numpy()\n",
    "\n",
    "# Compute and print gradients at x = -1, 1, and 0\n",
    "print(compute_gradient(-1.0))\n",
    "print(compute_gradient(1.0))\n",
    "print(compute_gradient(0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with image data\n",
    "You are given a black-and-white image of a letter, which has been encoded as a tensor, letter. You want to determine whether the letter is an X or a K. You don't have a trained neural network, but you do have a simple model, model, which can be used to classify letter.\n",
    "\n",
    "The 3x3 tensor, letter, and the 1x3 tensor, model, are available in the Python shell. You can determine whether letter is a K by multiplying letter by model, summing over the result, and then checking if it is equal to 1. As with more complicated models, such as neural networks, model is a collection of weights, arranged in a tensor.\n",
    "\n",
    "Note that the functions reshape(), matmul(), and reduce_sum() have been imported from tensorflow and are available for use.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- The model, model, is 1x3 tensor, but should be a 3x1. Reshape model.\n",
    "- Perform a matrix multiplication of the 3x3 tensor, letter, by the 3x1 tensor, model.\n",
    "- Sum over the resulting tensor, output, and assign this value to prediction.\n",
    "- Print prediction using the .numpy() method to determine whether letter is K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "letter = np.array([[1., 0., 1.], [1., 1., 0.], [1., 0., 1.]])\n",
    "model = np.array([[ 1.,  0., -1.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Reshape model from a 1x3 to a 3x1 tensor\n",
    "model = tf.reshape(model, (3, 1))\n",
    "\n",
    "# Multiply letter by model\n",
    "output = tf.matmul(letter, model)\n",
    "\n",
    "# Sum over output and print prediction using the numpy method\n",
    "prediction = tf.reduce_sum(output)\n",
    "print(prediction.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Linear models\n",
    "\n",
    "In this chapter, you will learn how to build, solve, and make predictions with models in TensorFlow 2.0. You will focus on a simple class of models – the linear regression model – and will try to predict housing prices. By the end of the chapter, you will know how to load and manipulate data, construct loss functions, perform minimization, make predictions, and reduce resource use with batch training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data using pandas\n",
    "Before you can train a machine learning model, you must first import data. There are several valid ways to do this, but for now, we will use a simple one-liner from pandas: pd.read_csv(). Recall from the video that the first argument specifies the path or URL. All other arguments are optional.\n",
    "\n",
    "In this exercise, you will import the King County housing dataset, which we will use to train a linear model later in the chapter.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Import pandas under the alias pd.\n",
    "- Assign the path to a string variable with the name data_path.\n",
    "- Load the dataset as a pandas dataframe named housing.\n",
    "- Print the price column of housing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        221900.0\n",
      "1        538000.0\n",
      "2        180000.0\n",
      "3        604000.0\n",
      "4        510000.0\n",
      "           ...   \n",
      "21608    360000.0\n",
      "21609    400000.0\n",
      "21610    402101.0\n",
      "21611    400000.0\n",
      "21612    325000.0\n",
      "Name: price, Length: 21613, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Assign the path to a string variable named data_path\n",
    "data_path = 'data/kc_house_data.csv'\n",
    "\n",
    "# Load the dataset as a dataframe named housing\n",
    "housing = pd.read_csv(data_path)\n",
    "\n",
    "# Print the price column of housing\n",
    "print(housing.price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the data type\n",
    "In this exercise, you will both load data and set its type. Note that housing is available and pandas has been imported as pd. You will import numpy and tensorflow, and define tensors that are usable in tensorflow using columns in housing with a given data type. Recall that you can select the price column, for instance, from housing using housing['price'].\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Import numpy and tensorflow under their standard aliases.\n",
    "- Use a numpy array to set the tensor price to have a data type of 32-bit floating point number\n",
    "- Use the tensorflow function cast() to set the tensor waterfront to have a - Boolean data type.\n",
    "- Print price and then waterfront. Did you notice any important differences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[221900. 538000. 180000. ... 402101. 400000. 325000.]\n",
      "tf.Tensor([False False False ... False False False], shape=(21613,), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Use a numpy array to define price as a 32-bit float\n",
    "price = np.array(housing['price'], np.float32)\n",
    "\n",
    "# Define waterfront as a Boolean using cast\n",
    "waterfront = tf.cast(housing['waterfront'], tf.bool)\n",
    "\n",
    "# Print price and waterfront\n",
    "print(price)\n",
    "print(waterfront)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions in TensorFlow\n",
    "In this exercise, you will compute the loss using data from the King County housing dataset. You are given a target, price, which is a tensor of house prices, and predictions, which is a tensor of predicted house prices. You will evaluate the loss function and print out the value of the loss.\n",
    "\n",
    "#### Instructions\n",
    "- Import the keras module from tensorflow. Then, use price and predictions to compute the mean squared error (mse).\n",
    "- Modify your code to compute the mean absolute error (mae), rather than the mean squared error (mse)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141307270000.0\n",
      "268921.97\n"
     ]
    }
   ],
   "source": [
    "predictions = tf.cast(housing.predictions, tf.float32)\n",
    "\n",
    "# Compute the mean squared error (mse)\n",
    "loss = keras.losses.mse(price, predictions)\n",
    "\n",
    "# Print the mean squared error (mse)\n",
    "print(loss.numpy())\n",
    "\n",
    "# Compute the mean absolute error (mae)\n",
    "loss = keras.losses.mae(price, predictions)\n",
    "\n",
    "# Print the mean absolute error (mae)\n",
    "print(loss.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying the loss function\n",
    "In the previous exercise, you defined a tensorflow loss function and then evaluated it once for a set of actual and predicted values. In this exercise, you will compute the loss within another function called loss_function(), which first generates predicted values from the data and variables. The purpose of this is to construct a function of the trainable model variables that returns the loss. You can then repeatedly evaluate this function for different variable values until you find the minimum. In practice, you will pass this function to an optimizer in tensorflow. Note that features and targets have been defined and are available. Additionally, Variable, float32, and keras are available.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Define a variable, scalar, with an initial value of 1.0 and a type of float32.\n",
    "- Define a function called loss_function(), which takes scalar, features, and targets as arguments in that order.\n",
    "- Use a mean absolute error loss function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = constant(np.array([ 2.,  4.,  6.,  8., 10.], dtype=np.float32))\n",
    "features = constant(np.array([1.,2.,3.,4.,5.]), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n"
     ]
    }
   ],
   "source": [
    "# Initialize a variable named scalar\n",
    "scalar = Variable(1.0, tf.float32)\n",
    "\n",
    "# Define the model\n",
    "def model(scalar, features = features):\n",
    "  \treturn scalar * features\n",
    "\n",
    "# Define a loss function\n",
    "def loss_function(scalar, features = features, targets = targets):\n",
    "\t# Compute the predicted values\n",
    "\tpredictions = model(scalar, features)\n",
    "    \n",
    "\t# Return the mean absolute error loss\n",
    "\treturn keras.losses.mae(targets, predictions)\n",
    "\n",
    "# Evaluate the loss function and print the loss\n",
    "print(loss_function(scalar).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up a linear regression\n",
    "A univariate linear regression identifies the relationship between a single feature and the target tensor. In this exercise, we will use a property's lot size and price. Just as we discussed in the video, we will take the natural logarithms of both tensors, which are available as price_log and size_log.\n",
    "\n",
    "In this exercise, you will define the model and the loss function. You will then evaluate the loss function for two different values of intercept and slope. Remember that the predicted values are given by intercept + features*slope. Additionally, note that keras.losses.mse() is available for you. Furthermore, slope and intercept have been defined as variables.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Define a function that returns the predicted values for a linear regression using intercept, features, and slope, and without using add() or multiply().\n",
    "- Complete the loss_function() by adding the model's variables, intercept and slope, as arguments.\n",
    "- Compute the mean squared error using targets and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_log = np.log(price)\n",
    "size_log = np.log(housing.sqft_lot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145.448336689515\n",
      "71.86728197085229\n"
     ]
    }
   ],
   "source": [
    "# Define a linear regression model\n",
    "def linear_regression(intercept, slope, features = size_log):\n",
    "\treturn intercept + slope * features\n",
    "\n",
    "# Set loss_function() to take the variables as arguments\n",
    "def loss_function(intercept, slope, features = size_log, targets = price_log):\n",
    "\t# Set the predicted values\n",
    "\tpredictions = linear_regression(intercept, slope, features)\n",
    "    \n",
    "    # Return the mean squared error loss\n",
    "\treturn keras.losses.mean_squared_error(targets,predictions)\n",
    "\n",
    "# Compute the loss for different slope and intercept values\n",
    "print(loss_function(0.1, 0.1).numpy())\n",
    "print(loss_function(0.1, 0.5).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a linear model\n",
    "In this exercise, we will pick up where the previous exercise ended. The intercept and slope, intercept and slope, have been defined and initialized. Additionally, a function has been defined, loss_function(intercept, slope), which computes the loss using the data and model variables.\n",
    "\n",
    "You will now define an optimization operation as opt. You will then train a univariate linear model by minimizing the loss to find the optimal values of intercept and slope. Note that the opt operation will try to move closer to the optimum with each step, but will require many steps to find it. Thus, you must repeatedly execute the operation.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Initialize an Adam optimizer as opt with a learning rate of 0.5.\n",
    "- Apply the .minimize() method to the optimizer.\n",
    "- Pass loss_function() with the appropriate arguments as a lambda function to .minimize().\n",
    "- Supply the list of variables that need to be updated to var_list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(intercept, slope):\n",
    "        size_range = np.linspace(6,14,100)\n",
    "        price_pred = [intercept+slope*s for s in size_range]\n",
    "        plt.scatter(size_log, price_log, color = 'black')\n",
    "        plt.plot(size_range, price_pred, linewidth=3.0, color='red')\n",
    "        plt.xlabel('log(size)')\n",
    "        plt.ylabel('log(price)')\n",
    "        plt.title('Scatterplot of data and fitted regression line')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.681749\n",
      "11.737402\n",
      "1.1300178\n",
      "1.6701479\n",
      "0.8092006\n",
      "0.812697\n",
      "0.6221346\n",
      "0.6119664\n",
      "0.5934977\n",
      "0.5704044\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "intercept = Variable(5.)\n",
    "slope = Variable(0.001)\n",
    "# Initialize an adam optimizer\n",
    "opt = keras.optimizers.Adam(0.5)\n",
    "\n",
    "for j in range(100):\n",
    "\t# Apply minimize, pass the loss function, and supply the variables\n",
    "\topt.minimize(lambda: loss_function(intercept, slope), var_list=[intercept, slope])\n",
    "\n",
    "\t# Print every 10th value of the loss\n",
    "\tif j % 10 == 0:\n",
    "\t\tprint(loss_function(intercept, slope).numpy())\n",
    "\n",
    "# Plot data and regression line\n",
    "plot_results(intercept, slope)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple linear regression\n",
    "In most cases, performing a univariate linear regression will not yield a model that is useful for making accurate predictions. In this exercise, you will perform a multiple regression, which uses more than one feature.\n",
    "\n",
    "You will use price_log as your target and size_log and bedrooms as your features. Each of these tensors has been defined and is available. You will also switch from using the the mean squared error loss to the mean absolute error loss: keras.losses.mae(). Finally, the predicted values are computed as follows: params[0] + feature1*params[1] + feature2*params[2]. Note that we've defined a vector of parameters, params, as a variable, rather than using three variables. Here, params[0] is the intercept and params[1] and params[2] are the slopes.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Define a linear regression model that returns the predicted values.\n",
    "- Set loss_function() to take the parameter vector as an input.\n",
    "- Use the mean absolute error loss.\n",
    "- Complete the minimization operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrooms = housing.bedrooms.to_numpy(dtype = np.float32 )\n",
    "params = Variable(np.array([0.1 , 0.05, 0.02], dtype=np.float32))\n",
    "\n",
    "def print_results(params):\n",
    "        return print('loss: {:0.3f}, intercept: {:0.3f}, slope_1: {:0.3f}, slope_2: {:0.3f}'\n",
    "                     .format(loss_function(params).numpy(), params[0].numpy(), params[1].numpy(), \n",
    "                             params[2].numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 12.427, intercept: 0.101, slope_1: 0.050, slope_2: 0.021\n",
      "loss: 12.422, intercept: 0.102, slope_1: 0.050, slope_2: 0.022\n",
      "loss: 12.418, intercept: 0.103, slope_1: 0.050, slope_2: 0.023\n",
      "loss: 12.413, intercept: 0.104, slope_1: 0.050, slope_2: 0.024\n",
      "loss: 12.409, intercept: 0.105, slope_1: 0.050, slope_2: 0.025\n",
      "loss: 12.405, intercept: 0.106, slope_1: 0.050, slope_2: 0.026\n",
      "loss: 12.400, intercept: 0.107, slope_1: 0.050, slope_2: 0.027\n",
      "loss: 12.396, intercept: 0.108, slope_1: 0.050, slope_2: 0.028\n",
      "loss: 12.392, intercept: 0.109, slope_1: 0.050, slope_2: 0.029\n",
      "loss: 12.387, intercept: 0.110, slope_1: 0.050, slope_2: 0.030\n"
     ]
    }
   ],
   "source": [
    "# Define the linear regression model\n",
    "def linear_regression(params, feature1 = size_log, feature2 = bedrooms):\n",
    "\treturn params[0] + feature1*params[1] + feature2*params[2]\n",
    "\n",
    "# Define the loss function\n",
    "def loss_function(params, targets = price_log, feature1 = size_log, feature2 = bedrooms):\n",
    "\t# Set the predicted values\n",
    "\tpredictions = linear_regression(params, feature1, feature2)\n",
    "  \n",
    "\t# Use the mean absolute error loss\n",
    "\treturn keras.losses.mae(targets, predictions)\n",
    "\n",
    "# Define the optimize operation\n",
    "opt = keras.optimizers.Adam()\n",
    "\n",
    "# Perform minimization and print trainable variables\n",
    "for j in range(10):\n",
    "\topt.minimize(lambda: loss_function(params), var_list=[params])\n",
    "\tprint_results(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing to batch train\n",
    "Before we can train a linear model in batches, we must first define variables, a loss function, and an optimization operation. In this exercise, we will prepare to train a model that will predict price_batch, a batch of house prices, using size_batch, a batch of lot sizes in square feet. In contrast to the previous lesson, we will do this by loading batches of data using pandas, converting it to numpy arrays, and then using it to minimize the loss function in steps.\n",
    "\n",
    "Variable(), keras(), and float32 have been imported for you. Note that you should not set default argument values for either the model or loss function, since we will generate the data in batches during the training process.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Define intercept as having an initial value of 10.0 and a data type of 32-bit float.\n",
    "- Define the model to return the predicted values using intercept, features, and slope.\n",
    "- Define a function called loss_function() that takes intercept, slope, targets, and features as arguments. Do not set default argument values.\n",
    "- Define the mean squared error loss function using targets and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the intercept and slope\n",
    "intercept = Variable(10.0, tf.float32)\n",
    "slope = Variable(0.5, tf.float32)\n",
    "\n",
    "# Define the model\n",
    "def linear_regression(intercept, slope, features):\n",
    "\t# Define the predicted values\n",
    "\treturn intercept + slope* features\n",
    "\n",
    "# Define the loss function\n",
    "def loss_function(intercept, slope, targets, features):\n",
    "\t# Define the predicted values\n",
    "\tpredictions = linear_regression(intercept, slope, features)\n",
    "    \n",
    " \t# Define the MSE loss\n",
    "\treturn keras.losses.mean_squared_error(targets, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a linear model in batches\n",
    "In this exercise, we will train a linear regression model in batches, starting where we left off in the previous exercise. We will do this by stepping through the dataset in batches and updating the model's variables, intercept and slope, after each step. This approach will allow us to train with datasets that are otherwise too large to hold in memory.\n",
    "\n",
    "Note that the loss function,loss_function(intercept, slope, targets, features), has been defined for you. Additionally, keras has been imported for you and numpy is available as np. The trainable variables should be entered into var_list in the order in which they appear as loss function arguments.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Use the .Adam() optimizer.\n",
    "- Load in the data from 'kc_house_data.csv' in batches with a chunksize of 100.\n",
    "- Extract the price column from batch, convert it to a numpy array of type 32-bit float, and assign it to price_batch.\n",
    "- Complete the loss function, fill in the list of trainable variables, and perform minimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.217884 ,  0.7015924\n"
     ]
    }
   ],
   "source": [
    "# Initialize adam optimizer\n",
    "opt = keras.optimizers.Adam()\n",
    "\n",
    "# Load data in batches\n",
    "for batch in pd.read_csv('data/kc_house_data.csv', chunksize = 100):\n",
    "\tsize_batch = np.array(batch['sqft_lot'], np.float32)\n",
    "\n",
    "\t# Extract the price values for the current batch\n",
    "\tprice_batch = np.array(batch['price'], np.float32)\n",
    "\n",
    "\t# Complete the loss, fill in the variable list, and minimize\n",
    "\topt.minimize(lambda: loss_function(intercept, slope, price_batch, size_batch), var_list=[intercept, slope])\n",
    "\n",
    "# Print trained parameters\n",
    "print(intercept.numpy(), ', ', slope.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Neural Networks\n",
    "\n",
    "The previous chapters taught you how to build models in TensorFlow 2.0. In this chapter, you will apply those same tools to build, train, and make predictions with neural networks. You will learn how to define dense layers, apply activation functions, select an optimizer, and apply regularization to reduce overfitting. You will take advantage of TensorFlow's flexibility by using both low-level linear algebra and high-level Keras API operations to define and train models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The linear algebra of dense layers\n",
    "There are two ways to define a dense layer in tensorflow. The first involves the use of low-level, linear algebraic operations. The second makes use of high-level keras operations. In this exercise, we will use the first method to construct the network shown in the image below.\n",
    "\n",
    "<img src=\"3_2_1_network2.png\" width=200  >\n",
    "\n",
    "This image depicts an neural network with 5 input nodes and 3 output nodes.\n",
    "The input layer contains 3 features -- education, marital status, and age -- which are available as borrower_features. The hidden layer contains 2 nodes and the output layer contains a single node.\n",
    "\n",
    "For each layer, you will take the previous layer as an input, initialize a set of weights, compute the product of the inputs and weights, and then apply an activation function. Note that Variable(), ones(), matmul(), and keras() have been imported from tensorflow.\n",
    "\n",
    "#### Instructions\n",
    "- Initialize weights1 as a variable using a 3x2 tensor of ones.\n",
    "- Compute the product of borrower_features by weights1 using matrix multiplication.\n",
    "- Use a sigmoid activation function to transform product1 + bias1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " dense1's output shape: (1, 2)\n"
     ]
    }
   ],
   "source": [
    "borrower_features = np.array([[ 2.,  1., 24.]], dtype=np.float32)\n",
    "# Initialize bias1\n",
    "bias1 = Variable(1.0)\n",
    "\n",
    "# Initialize weights1 as 3x2 variable of ones\n",
    "weights1 = Variable(tf.ones((3, 2)))\n",
    "\n",
    "# Perform matrix multiplication of borrower_features and weights1\n",
    "product1 = tf.matmul(borrower_features,weights1)\n",
    "\n",
    "# Apply sigmoid activation function to product1 + bias1\n",
    "dense1 = keras.activations.sigmoid(product1  + bias1)\n",
    "\n",
    "# Print shape of dense1\n",
    "print(\"\\n dense1's output shape: {}\".format(dense1.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initialize weights2 as a variable using a 2x1 tensor of ones.\n",
    "- Compute the product of dense1 by weights2 using matrix multiplication.\n",
    "- Use a sigmoid activation function to transform product2 + bias2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " prediction: 0.9525741338729858\n",
      "\n",
      " actual: 1\n"
     ]
    }
   ],
   "source": [
    "# Initialize bias2 and weights2\n",
    "bias2 = Variable(1.0)\n",
    "weights2 = Variable(tf.ones((2, 1)))\n",
    "\n",
    "# Perform matrix multiplication of dense1 and weights2\n",
    "product2 = tf.matmul(dense1,weights2)\n",
    "\n",
    "# Apply activation to product2 + bias2 and print the prediction\n",
    "prediction = keras.activations.sigmoid(product2 + bias2)\n",
    "print('\\n prediction: {}'.format(prediction.numpy()[0,0]))\n",
    "print('\\n actual: 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The low-level approach with multiple examples\n",
    "In this exercise, we'll build further intuition for the low-level approach by constructing the first dense hidden layer for the case where we have multiple examples. We'll assume the model is trained and the first layer weights, weights1, and bias, bias1, are available. We'll then perform matrix multiplication of the borrower_features tensor by the weights1 variable. Recall that the borrower_features tensor includes education, marital status, and age. Finally, we'll apply the sigmoid function to the elements of products1 + bias1, yielding dense1.\n",
    "\n",
    "<img src=\"twolevelapp.JPG\" width=300 >\n",
    "\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Compute products1 by matrix multiplying the features tensor by the weights.\n",
    "- Use a sigmoid activation function to transform products1 + bias1.\n",
    "- Print the shapes of borrower_features, weights1, bias1, and dense1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "matmul = tf.matmul\n",
    "float32 = np.float32\n",
    "\n",
    "borrower_features = Variable(np.array([[ 3.,  3., 23.],\n",
    "                              [ 2.,  1., 24.],\n",
    "                              [ 1.,  1., 49.],\n",
    "                              [ 1.,  1., 49.],\n",
    "                              [ 2.,  1., 29.]], dtype=np.float32))\n",
    "\n",
    "weights1 = Variable(np.array([[-0.6 ,  0.6 ],\n",
    "                     [ 0.8 , -0.3 ],\n",
    "                     [-0.09, -0.08]], dtype=np.float32))\n",
    "\n",
    "bias1 = Variable([0.1], dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " shape of borrower_features:  (5, 3)\n",
      "\n",
      " shape of weights1:  (3, 2)\n",
      "\n",
      " shape of bias1:  (1,)\n",
      "\n",
      " shape of dense1:  (5, 2)\n"
     ]
    }
   ],
   "source": [
    "# Compute the product of borrower_features and weights1\n",
    "products1 = matmul(borrower_features, weights1)\n",
    "\n",
    "# Apply a sigmoid activation function to products1 + bias1\n",
    "dense1 = keras.activations.sigmoid(products1 + bias1)\n",
    "\n",
    "# Print the shapes of borrower_features, weights1, bias1, and dense1\n",
    "print('\\n shape of borrower_features: ', borrower_features.shape)\n",
    "print('\\n shape of weights1: ', weights1.shape)\n",
    "print('\\n shape of bias1: ', bias1.shape)\n",
    "print('\\n shape of dense1: ', dense1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the dense layer operation\n",
    "We've now seen how to define dense layers in tensorflow using linear algebra. In this exercise, we'll skip the linear algebra and let keras work out the details. This will allow us to construct the network below, which has 2 hidden layers and 10 features, using less code than we needed for the network with 1 hidden layer and 3 features.\n",
    "\n",
    "<img src=\"dense_network.png\" width = 350  >\n",
    "\n",
    "To construct this network, we'll need to define three dense layers, each of which takes the previous layer as an input, multiplies it by weights, and applies an activation function. Note that input data has been defined and is available as a 100x10 tensor: borrower_features. Additionally, the keras.layers module is available.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Set dense1 to be a dense layer with 7 output nodes and a sigmoid activation function.\n",
    "- Define dense2 to be dense layer with 3 output nodes and a sigmoid activation function.\n",
    "- Define predictions to be a dense layer with 1 output node and a sigmoid activation function.\n",
    "- Print the shapes of dense1, dense2, and predictions in that order using the .shape method. Why does each of these tensors have 100 rows?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "borrower_features = Variable(pd.read_csv('data/lit_uci_credit_card.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " shape of dense1:  (100, 7)\n",
      "\n",
      " shape of dense2:  (100, 3)\n",
      "\n",
      " shape of predictions:  (100, 1)\n"
     ]
    }
   ],
   "source": [
    "# Define the first dense layer\n",
    "dense1 = keras.layers.Dense(7, activation='sigmoid')(borrower_features)\n",
    "\n",
    "# Define a dense layer with 3 output nodes\n",
    "dense2 = keras.layers.Dense(3, activation='sigmoid')(dense1)\n",
    "\n",
    "# Define a dense layer with 1 output node\n",
    "predictions = keras.layers.Dense(1, activation='sigmoid')(dense2)\n",
    "\n",
    "# Print the shapes of dense1, dense2, and predictions\n",
    "print('\\n shape of dense1: ', dense1.shape)\n",
    "print('\\n shape of dense2: ', dense2.shape)\n",
    "print('\\n shape of predictions: ', predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary classification problems\n",
    "In this exercise, you will again make use of credit card data. The target variable, default, indicates whether a credit card holder defaults on his or her payment in the following period. Since there are only two options--default or not--this is a binary classification problem. While the dataset has many features, you will focus on just three: the size of the three latest credit card bills. Finally, you will compute predictions from your untrained network, outputs, and compare those the target variable, default.\n",
    "\n",
    "The tensor of features has been loaded and is available as bill_amounts. Additionally, the constant(), float32, and keras.layers.Dense() operations are available.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Define inputs as a 32-bit floating point constant tensor using bill_amounts.\n",
    "- Set dense1 to be a dense layer with 3 output nodes and a relu activation function.\n",
    "- Set dense2 to be a dense layer with 2 output nodes and a relu activation function.\n",
    "- Set the output layer to be a dense layer with a single output node and a sigmoid activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"BILL_AMT1\",\"BILL_AMT2\",\"BILL_AMT3\"]\n",
    "bill_amounts = pd.read_csv('data/uci_credit_card.csv', \n",
    "                           usecols = cols).to_numpy()\n",
    "default = pd.read_csv('data/uci_credit_card.csv', \n",
    "                           usecols = ['default']).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5]\n",
      " [ 0.5]\n",
      " [-0.5]\n",
      " [-0.5]\n",
      " [ 0. ]]\n"
     ]
    }
   ],
   "source": [
    "# Construct input layer from features\n",
    "inputs = constant(bill_amounts, dtype=float32)\n",
    "\n",
    "# Define first dense layer\n",
    "dense1 = keras.layers.Dense(3, activation='relu')(inputs)\n",
    "\n",
    "# Define second dense layer\n",
    "dense2 = keras.layers.Dense(2, activation='relu')(dense1)\n",
    "\n",
    "# Define output layer\n",
    "outputs = keras.layers.Dense(1, activation='sigmoid')(dense2)\n",
    "\n",
    "# Print error for first five examples\n",
    "error = default[:5] - outputs.numpy()[:5]\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass classification problems\n",
    "In this exercise, we expand beyond binary classification to cover multiclass problems. A multiclass problem has targets that can take on three or more values. In the credit card dataset, the education variable can take on 6 different values, each corresponding to a different level of education. We will use that as our target in this exercise and will also expand the feature set from 3 to 10 columns.\n",
    "\n",
    "As in the previous problem, you will define an input layer, dense layers, and an output layer. You will also print the untrained model's predictions, which are probabilities assigned to the classes. The tensor of features has been loaded and is available as borrower_features. Additionally, the constant(), float32, and keras.layers.Dense() operations are available.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Define the input layer as a 32-bit constant tensor using borrower_features.\n",
    "- Set the first dense layer to have 10 output nodes and a sigmoid activation function.\n",
    "- Set the second dense layer to have 8 output nodes and a rectified linear unit activation function.\n",
    "- Set the output layer to have 6 output nodes and the appropriate activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "borrower_features = np.loadtxt('data/borrower_features.csv', \n",
    "                               delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1351358  0.2215386  0.14394125 0.15181434 0.18815282 0.15941717]\n",
      " [0.11984477 0.16559665 0.21495858 0.13474388 0.27257714 0.092279  ]\n",
      " [0.14376889 0.22994697 0.14290838 0.13192382 0.21024515 0.14120682]\n",
      " [0.1351358  0.2215386  0.14394125 0.15181434 0.18815282 0.15941717]\n",
      " [0.18209739 0.19499354 0.14169827 0.12716864 0.23951651 0.11452567]]\n"
     ]
    }
   ],
   "source": [
    "# Construct input layer from borrower features\n",
    "inputs = constant(borrower_features, dtype=np.float32)\n",
    "\n",
    "# Define first dense layer\n",
    "dense1 = keras.layers.Dense(10, activation='sigmoid')(inputs)\n",
    "\n",
    "# Define second dense layer\n",
    "dense2 = keras.layers.Dense(8, activation='relu')(dense1)\n",
    "\n",
    "# Define output layer\n",
    "outputs = keras.layers.Dense(6, activation='softmax')(dense2)\n",
    "\n",
    "# Print first five predictions\n",
    "print(outputs.numpy()[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dangers of local minima\n",
    "Consider the plot of the following loss function, loss_function(), which contains a global minimum, marked by the dot on the right, and several local minima, including the one marked by the dot on the left.\n",
    "\n",
    "<img src=\"local_minima.png\" width = 280>\n",
    "\n",
    "The graph is of a single variable function that contains multiple local minima and a global minimum.\n",
    "\n",
    "In this exercise, you will try to find the global minimum of loss_function() using keras.optimizers.SGD(). You will do this twice, each time with a different initial value of the input to loss_function(). First, you will use x_1, which is a variable with an initial value of 6.0. Second, you will use x_2, which is a variable with an initial value of 0.3. Note that loss_function() has been defined and is available.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Set opt to use the stochastic gradient descent optimizer (SGD) with a learning rate of 0.01.\n",
    "- Perform minimization using the loss function, loss_function(), and the variable with an initial value of 6.0, x_1.\n",
    "- Perform minimization using the loss function, loss_function(), and the variable with an initial value of 0.3, x_2.\n",
    "- Print x_1 and x_2 as numpy arrays and check whether the values differ. - These are the minima that the algorithm identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def loss_function(x):\n",
    "        return 4.0*math.cos(x-1)+tf.divide(math.cos(2.0*np.pi*x),x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2adc2172a88>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xx = np.linspace(0.1,8,300)\n",
    "yy = [loss_function(x) for x in xx]\n",
    "plt.plot(xx, yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.121951 0.25\n"
     ]
    }
   ],
   "source": [
    "# Initialize x_1 and x_2\n",
    "x_1 = tf.Variable(6.0,float32)\n",
    "x_2 = tf.Variable(0.3,float32)\n",
    "\n",
    "# Define the optimization operation\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "for j in range(500):\n",
    "\t# Perform minimization using the loss function and x_1\n",
    "\topt.minimize(lambda: loss_function(x_1), var_list=[x_1])\n",
    "\t# Perform minimization using the loss function and x_2\n",
    "\topt.minimize(lambda: loss_function(x_2), var_list=[x_2])\n",
    "\n",
    "# Print x_1 and x_2 as numpy arrays\n",
    "print(x_1.numpy(), x_2.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avoiding local minima\n",
    "The previous problem showed how easy it is to get stuck in local minima. We had a simple optimization problem in one variable and gradient descent still failed to deliver the global minimum when we had to travel through local minima first. One way to avoid this problem is to use momentum, which allows the optimizer to break through local minima. We will again use the loss function from the previous problem, which has been defined and is available for you as loss_function().\n",
    "\n",
    "\n",
    "Several optimizers in tensorflow have a momentum parameter, including SGD and RMSprop. You will make use of RMSprop in this exercise. Note that x_1 and x_2 have been initialized to the same value this time. Furthermore, keras.optimizers.RMSprop() has also been imported for you from tensorflow.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Set the opt_1 operation to use a learning rate of 0.01 and a momentum of 0.99.\n",
    "- Set opt_2 to use the root mean square propagation (RMS) optimizer with a learning rate of 0.01 and a momentum of 0.00.\n",
    "- Define the minimization operation for opt_2.\n",
    "- Print x_1 and x_2 as numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.2598577 0.24519981\n"
     ]
    }
   ],
   "source": [
    "# Initialize x_1 and x_2\n",
    "x_1 = Variable(0.05,float32)\n",
    "x_2 = Variable(0.05,float32)\n",
    "\n",
    "# Define the optimization operation for opt_1 and opt_2\n",
    "opt_1 = keras.optimizers.RMSprop(learning_rate=0.01, momentum=0.995)\n",
    "opt_2 = keras.optimizers.RMSprop(learning_rate=0.01, momentum=0.00)\n",
    "\n",
    "for j in range(500):\n",
    "\topt_1.minimize(lambda: loss_function(x_1), var_list=[x_1])\n",
    "    # Define the minimization operation for opt_2\n",
    "\topt_2.minimize(lambda: loss_function(x_2), var_list=[x_2])\n",
    "\n",
    "# Print x_1 and x_2 as numpy arrays\n",
    "print(x_1.numpy(), x_2.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization in TensorFlow\n",
    "A good initialization can reduce the amount of time needed to find the global minimum. In this exercise, we will initialize weights and biases for a neural network that will be used to predict credit card default decisions. To build intuition, we will use the low-level, linear algebraic approach, rather than making use of convenience functions and high-level keras operations. We will also expand the set of input features from 3 to 23. \n",
    "#### Instructions\n",
    "\n",
    "- Initialize the layer 1 weights, w1, as a Variable() with shape [23, 7], drawn from a normal distribution.\n",
    "- Initialize the layer 1 bias using ones.\n",
    "- Use a draw from the normal distribution to initialize w2 as a Variable() with shape [7, 1].\n",
    "- Define b2 as a Variable() and set its initial value to 0.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the layer 1 weights\n",
    "w1 = Variable(tf.random.normal([23, 7]))\n",
    "\n",
    "# Initialize the layer 1 bias\n",
    "b1 = Variable(tf.random.normal([7]))\n",
    "\n",
    "# Define the layer 2 weights\n",
    "w2 = Variable(tf.random.normal([7, 1]))\n",
    "\n",
    "# Define the layer 2 bias\n",
    "b2 = Variable(tf.random.normal([1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model and loss function\n",
    "In this exercise, you will train a neural network to predict whether a credit card holder will default. The features and targets you will use to train your network are available in the Python shell as borrower_features and default. You defined the weights and biases in the previous exercise.\n",
    "\n",
    "Note that the predictions layer is defined as σ(layer1∗w2+b2), where σ is the sigmoid activation, layer1 is a tensor of nodes for the first hidden dense layer, w2 is a tensor of weights, and b2 is the bias tensor.\n",
    "\n",
    "The trainable variables are w1, b1, w2, and b2. Additionally, the following operations have been imported for you: keras.activations.relu() and keras.layers.Dropout().\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Apply a rectified linear unit activation function to the first layer.\n",
    "- Apply 25% dropout to layer1.\n",
    "- Pass the target, targets, and the predicted values, predictions, to the cross entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/borrow.csv', header=None)\n",
    "borrower_features = df.iloc[:2500,:-1].to_numpy().astype('float32')\n",
    "default = df.iloc[:2500,-1].to_numpy().reshape([2500,1]).astype('float32')\n",
    "\n",
    "test_features = df.iloc[2500:,:-1].to_numpy().astype('float32')\n",
    "test_targets = df.iloc[2500:,-1].to_numpy().reshape([500,1]).astype('float32')\n",
    "\n",
    "# Define the model\n",
    "def model(w1, b1, w2, b2, features = borrower_features):\n",
    "\t# Apply relu activation functions to layer 1\n",
    "\tlayer1 = keras.activations.relu(tf.matmul(features, w1) + b1)\n",
    "    # Apply dropout\n",
    "\tdropout = keras.layers.Dropout(0.25)(layer1)\n",
    "\treturn keras.activations.sigmoid(tf.matmul(dropout, w2) + b2)\n",
    "\n",
    "# Define the loss function\n",
    "def loss_function(w1, b1, w2, b2, features = borrower_features, targets = default):\n",
    "\tpredictions = model(w1, b1, w2, b2)\n",
    "\t# Pass targets and predictions to the cross entropy loss\n",
    "\treturn keras.losses.binary_crossentropy(targets, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training neural networks with TensorFlow\n",
    "In the previous exercise, you defined a model, model(w1, b1, w2, b2, features), and a loss function, loss_function(w1, b1, w2, b2, features, targets), both of which are available to you in this exercise. You will now train the model and then evaluate its performance by predicting default outcomes in a test set, which consists of test_features and test_targets and is available to you. The trainable variables are w1, b1, w2, and b2. Additionally, the following operations have been imported for you: keras.activations.relu() and keras.layers.Dropout().\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Set the optimizer to perform minimization.\n",
    "- Add the four trainable variables to var_list in the order in which they appear as arguments to loss_function().\n",
    "- Use the model and test_features to predict the values for test_targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt= tf.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500, 1)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seaborn import heatmap\n",
    "def confusion_matrix(default, model_predictions):\n",
    "        df = pd.DataFrame(np.hstack([default, model_predictions.numpy() > 0.5]), \n",
    "                       columns = ['Actual','Predicted'])\n",
    "        confusion_matrix = pd.crosstab(df['Actual'], df['Predicted'], \n",
    "                                    rownames=['Actual'], colnames=['Predicted'])\n",
    "        heatmap(confusion_matrix, cmap=\"Greys\", fmt=\"d\", annot=True, cbar=False)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model\n",
    "for j in range(100):\n",
    "    # Complete the optimizer\n",
    "\topt.minimize(lambda: loss_function(w1, b1, w2, b2), \n",
    "                 var_list=[w1, b1, w2, b2])\n",
    "\n",
    "# Make predictions with model\n",
    "model_predictions = model(w1, b1, w2, b2, test_features)\n",
    "\n",
    "# Construct the confusion matrix\n",
    "confusion_matrix(test_targets, model_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. High Level APIs\n",
    "\n",
    "In the final chapter, you'll use high-level APIs in TensorFlow 2.0 to train a sign language letter classifier. You will use both the sequential and functional Keras APIs to train, validate, make predictions with, and evaluate models. You will also learn how to use the Estimators API to streamline the model definition and training process, and to avoid errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 16)                12560     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 4)                 36        \n",
      "=================================================================\n",
      "Total params: 12,732\n",
      "Trainable params: 12,732\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define a Keras sequential model\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Define the first dense layer\n",
    "model.add(keras.layers.Dense(16, activation='relu', input_shape=(784,)))\n",
    "\n",
    "# Define the second dense layer\n",
    "model.add(keras.layers.Dense(8, activation = 'relu'))\n",
    "\n",
    "# Define the output layer\n",
    "model.add(keras.layers.Dense(4, activation= 'softmax'))\n",
    "\n",
    "# Print the model architecture\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling a sequential model\n",
    "In this exercise, you will work towards classifying letters from the Sign Language MNIST dataset; however, you will adopt a different network architecture than what you used in the previous exercise. There will be fewer layers, but more nodes. You will also apply dropout to prevent overfitting. Finally, you will compile the model to use the adam optimizer and the categorical_crossentropy loss. You will also use a method in keras to summarize your model's architecture. Note that keras has been imported from tensorflow for you and a sequential keras model has been defined as model.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- In the first dense layer, set the number of nodes to 16, the activation to sigmoid, and the input_shape to (784,).\n",
    "- Apply dropout at a rate of 25% to the first layer's output.\n",
    "- Set the output layer to be dense, have 4 nodes, and use a softmax activation function.\n",
    "- Compile the model using an adam optimizer and categorical_crossentropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 16)                12560     \n",
      "_________________________________________________________________\n",
      "dropout_101 (Dropout)        (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 4)                 68        \n",
      "=================================================================\n",
      "Total params: 12,628\n",
      "Trainable params: 12,628\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model1 = keras.Sequential()\n",
    "# Define the first dense layer\n",
    "model1.add(keras.layers.Dense(16, activation='sigmoid', input_shape = (784,)))\n",
    "\n",
    "# Apply dropout to the first layer's output\n",
    "model1.add(keras.layers.Dropout(0.25))\n",
    "\n",
    "# Define the output layer\n",
    "model1.add(keras.layers.Dense(4, activation = 'softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model1.compile('adam', loss='categorical_crossentropy')\n",
    "\n",
    "# Print a model summary\n",
    "print(model1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a multiple input model\n",
    "In some cases, the sequential API will not be sufficiently flexible to accommodate your desired model architecture and you will need to use the functional API instead. If, for instance, you want to train two models with different architectures jointly, you will need to use the functional API to do this. In this exercise, we will see how to do this. We will also use the .summary() method to examine the joint model's architecture.\n",
    "\n",
    "Note that keras has been imported from tensorflow for you. Additionally, the input layers of the first and second models have been defined as m1_inputs and m2_inputs, respectively. Note that the two models have the same architecture, but one of them uses a sigmoid activation in the first layer and the other uses a relu.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Pass model 1's input layer to its first layer and model 1's first layer to its second layer.\n",
    "- Pass model 2's input layer to its first layer and model 2's first layer to its second layer.\n",
    "- Use the add() operation to combine the second layers of model 1 and model 2.\n",
    "- Complete the functional model definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1_inputs = tf.keras.Input(shape = (784))\n",
    "m2_inputs = tf.keras.Input(shape = (784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 784)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 784)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 12)           9420        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 12)           9420        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 4)            52          dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 4)            52          dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 4)            0           dense_15[0][0]                   \n",
      "                                                                 dense_17[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 18,944\n",
      "Trainable params: 18,944\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# For model 1, pass the input layer to layer 1 and layer 1 to layer 2\n",
    "m1_layer1 = keras.layers.Dense(12, activation='sigmoid')(m1_inputs)\n",
    "m1_layer2 = keras.layers.Dense(4, activation='softmax')(m1_layer1)\n",
    "\n",
    "# For model 2, pass the input layer to layer 1 and layer 1 to layer 2\n",
    "m2_layer1 = keras.layers.Dense(12, activation='relu')(m2_inputs)\n",
    "m2_layer2 = keras.layers.Dense(4, activation='softmax')(m2_layer1)\n",
    "\n",
    "# Merge model outputs and define a functional model\n",
    "merged = keras.layers.add([m1_layer2, m2_layer2])\n",
    "model = keras.Model(inputs=[m1_inputs, m2_inputs], outputs=merged)\n",
    "\n",
    "# Print a model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with Keras\n",
    "In this exercise, we return to our sign language letter classification problem. We have 2000 images of four letters--A, B, C, and D--and we want to classify them with a high level of accuracy. We will complete all parts of the problem, including the model definition, compilation, and training.\n",
    "\n",
    "Note that keras has been imported from tensorflow for you. Additionally, the features are available as sign_language_features and the targets are available as sign_language_labels.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Define a sequential model named model.\n",
    "- Set the output layer to be dense, have 4 nodes, and use a softmax activation function.\n",
    "- Compile the model with the SGD optimizer and categorical_crossentropy loss.\n",
    "- Complete the fitting operation and set the number of epochs to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_df = pd.read_csv('data/slmnist.csv', header=None)\n",
    "sign_language_features = mnist_df.iloc[:,1:].to_numpy().astype('float32')/255\n",
    "sign_language_labels = pd.get_dummies(mnist_df.iloc[:,0]).to_numpy().astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2960\n",
      "Epoch 2/5\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0951\n",
      "Epoch 3/5\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.8595\n",
      "Epoch 4/5\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.6656\n",
      "Epoch 5/5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.5278\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2adc2eff3c8>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a sequential model\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Define a hidden layer\n",
    "model.add(keras.layers.Dense(16, activation='relu', input_shape=(784,)))\n",
    "\n",
    "# Define the output layer\n",
    "model.add(keras.layers.Dense(4,activation = 'softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile('SGD', loss='categorical_crossentropy')\n",
    "\n",
    "# Complete the fitting operation\n",
    "model.fit(sign_language_features, sign_language_labels, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics and validation with Keras\n",
    "We trained a model to predict sign language letters in the previous exercise, but it is unclear how successful we were in doing so. In this exercise, we will try to improve upon the interpretability of our results. Since we did not use a validation split, we only observed performance improvements within the training set; however, it is unclear how much of that was due to overfitting. Furthermore, since we did not supply a metric, we only saw decreases in the loss function, which do not have any clear interpretation.\n",
    "\n",
    "Note that keras has been imported for you from tensorflow.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Set the first dense layer to have 32 nodes, use a sigmoid activation function, and have an input shape of (784,).\n",
    "- Use the root mean square propagation optimizer, a categorical crossentropy loss, and the accuracy metric.\n",
    "- Set the number of epochs to 10 and use 10% of the dataset for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "57/57 [==============================] - 1s 12ms/step - loss: 1.1168 - accuracy: 0.5739 - val_loss: 1.0051 - val_accuracy: 0.5350\n",
      "Epoch 2/30\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.7546 - accuracy: 0.8161 - val_loss: 0.6265 - val_accuracy: 0.9700\n",
      "Epoch 3/30\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.5752 - accuracy: 0.8867 - val_loss: 0.4949 - val_accuracy: 0.9700\n",
      "Epoch 4/30\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.4479 - accuracy: 0.9144 - val_loss: 0.4661 - val_accuracy: 0.8650\n",
      "Epoch 5/30\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.3542 - accuracy: 0.9439 - val_loss: 0.4153 - val_accuracy: 0.7800\n",
      "Epoch 6/30\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.2885 - accuracy: 0.9567 - val_loss: 0.2534 - val_accuracy: 0.9650\n",
      "Epoch 7/30\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.2390 - accuracy: 0.9667 - val_loss: 0.2159 - val_accuracy: 0.9650\n",
      "Epoch 8/30\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.1940 - accuracy: 0.9728 - val_loss: 0.1580 - val_accuracy: 0.9800\n",
      "Epoch 9/30\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.1661 - accuracy: 0.9806 - val_loss: 0.1922 - val_accuracy: 0.9400\n",
      "Epoch 10/30\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.1389 - accuracy: 0.9772 - val_loss: 0.1302 - val_accuracy: 0.9850\n",
      "Epoch 11/30\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.1159 - accuracy: 0.9867 - val_loss: 0.1255 - val_accuracy: 0.9900\n",
      "Epoch 12/30\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0981 - accuracy: 0.9861 - val_loss: 0.0769 - val_accuracy: 1.0000\n",
      "Epoch 13/30\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0825 - accuracy: 0.9889 - val_loss: 0.0688 - val_accuracy: 0.9850\n",
      "Epoch 14/30\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0676 - accuracy: 0.9906 - val_loss: 0.0780 - val_accuracy: 0.9950\n",
      "Epoch 15/30\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0661 - accuracy: 0.9883 - val_loss: 0.0572 - val_accuracy: 0.9900\n",
      "Epoch 16/30\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0520 - accuracy: 0.9928 - val_loss: 0.0385 - val_accuracy: 1.0000\n",
      "Epoch 17/30\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0455 - accuracy: 0.9939 - val_loss: 0.0380 - val_accuracy: 1.0000\n",
      "Epoch 18/30\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0421 - accuracy: 0.9922 - val_loss: 0.0324 - val_accuracy: 1.0000\n",
      "Epoch 19/30\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0332 - accuracy: 0.9978 - val_loss: 0.1921 - val_accuracy: 0.9200\n",
      "Epoch 20/30\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0317 - accuracy: 0.9944 - val_loss: 0.0215 - val_accuracy: 1.0000\n",
      "Epoch 21/30\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0252 - accuracy: 0.9967 - val_loss: 0.0276 - val_accuracy: 1.0000\n",
      "Epoch 22/30\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0248 - accuracy: 0.9967 - val_loss: 0.0326 - val_accuracy: 0.9950\n",
      "Epoch 23/30\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0220 - accuracy: 0.9961 - val_loss: 0.0721 - val_accuracy: 0.9750\n",
      "Epoch 24/30\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0173 - accuracy: 0.9972 - val_loss: 0.0120 - val_accuracy: 1.0000\n",
      "Epoch 25/30\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0178 - accuracy: 0.9978 - val_loss: 0.0101 - val_accuracy: 1.0000\n",
      "Epoch 26/30\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0142 - accuracy: 0.9994 - val_loss: 0.0131 - val_accuracy: 1.0000\n",
      "Epoch 27/30\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.0152 - accuracy: 0.9972 - val_loss: 0.0123 - val_accuracy: 1.0000\n",
      "Epoch 28/30\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0124 - accuracy: 0.9978 - val_loss: 0.0073 - val_accuracy: 1.0000\n",
      "Epoch 29/30\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0089 - accuracy: 0.9994 - val_loss: 0.0065 - val_accuracy: 1.0000\n",
      "Epoch 30/30\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0127 - accuracy: 0.9967 - val_loss: 0.0052 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2adc2154148>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define sequential model\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Define the first layer\n",
    "model.add(keras.layers.Dense(32, activation = 'sigmoid', input_shape = (784,)))\n",
    "\n",
    "# Add activation function to classifier\n",
    "model.add(keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "# Set the optimizer, loss function, and metrics\n",
    "model.compile(optimizer='RMSprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Add the number of epochs and the validation split\n",
    "model.fit(sign_language_features, sign_language_labels, epochs=30, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting detection\n",
    "In this exercise, we'll work with a small subset of the examples from the original sign language letters dataset. A small sample, coupled with a heavily-parameterized model, will generally lead to overfitting. This means that your model will simply memorize the class of each example, rather than identifying features that generalize to many examples.\n",
    "\n",
    "You will detect overfitting by checking whether the validation sample loss is substantially higher than the training sample loss and whether it increases with further training. With a small sample and a high learning rate, the model will struggle to converge on an optimum. You will set a low learning rate for the optimizer, which will make it easier to identify overfitting.\n",
    "\n",
    "Note that keras has been imported from tensorflow.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Define a sequential model in keras named model.\n",
    "- Add a first dense layer with 1024 nodes, a relu activation, and an input shape of (784,).\n",
    "- Set the learning rate to 0.01.\n",
    "- Set the fit() operation to iterate over the full sample 200 times and use 50% of the sample for validation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "32/32 [==============================] - 1s 40ms/step - loss: 11.4177 - accuracy: 0.3380 - val_loss: 0.9692 - val_accuracy: 0.7490\n",
      "Epoch 2/20\n",
      "32/32 [==============================] - 1s 37ms/step - loss: 0.7079 - accuracy: 0.7340 - val_loss: 0.6592 - val_accuracy: 0.7150\n",
      "Epoch 3/20\n",
      "32/32 [==============================] - 1s 37ms/step - loss: 0.3290 - accuracy: 0.9060 - val_loss: 0.2579 - val_accuracy: 0.9110ss: 0.3676 - accura\n",
      "Epoch 4/20\n",
      "32/32 [==============================] - 2s 57ms/step - loss: 0.1365 - accuracy: 0.9830 - val_loss: 0.1228 - val_accuracy: 0.9770\n",
      "Epoch 5/20\n",
      "32/32 [==============================] - 2s 47ms/step - loss: 0.0756 - accuracy: 0.9910 - val_loss: 0.0843 - val_accuracy: 0.9860\n",
      "Epoch 6/20\n",
      "32/32 [==============================] - 2s 53ms/step - loss: 0.0497 - accuracy: 0.9960 - val_loss: 0.0498 - val_accuracy: 0.9870\n",
      "Epoch 7/20\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 0.0273 - accuracy: 0.9970 - val_loss: 0.0353 - val_accuracy: 0.9910\n",
      "Epoch 8/20\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 0.0214 - accuracy: 0.9980 - val_loss: 0.0349 - val_accuracy: 0.9930\n",
      "Epoch 9/20\n",
      "32/32 [==============================] - 1s 33ms/step - loss: 0.0282 - accuracy: 0.9960 - val_loss: 0.0321 - val_accuracy: 0.9930\n",
      "Epoch 10/20\n",
      "32/32 [==============================] - 1s 33ms/step - loss: 0.0125 - accuracy: 0.9980 - val_loss: 0.0247 - val_accuracy: 0.9930\n",
      "Epoch 11/20\n",
      "32/32 [==============================] - 1s 33ms/step - loss: 0.0095 - accuracy: 1.0000 - val_loss: 0.0182 - val_accuracy: 0.9920\n",
      "Epoch 12/20\n",
      "32/32 [==============================] - 1s 35ms/step - loss: 0.0088 - accuracy: 0.9990 - val_loss: 0.0168 - val_accuracy: 0.9930\n",
      "Epoch 13/20\n",
      "32/32 [==============================] - 1s 32ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.0274 - val_accuracy: 0.9930\n",
      "Epoch 14/20\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.0156 - val_accuracy: 0.9930\n",
      "Epoch 15/20\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.0162 - val_accuracy: 0.9930\n",
      "Epoch 16/20\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.0064 - accuracy: 0.9980 - val_loss: 0.0223 - val_accuracy: 0.9930\n",
      "Epoch 17/20\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 0.0080 - accuracy: 0.9990 - val_loss: 0.0172 - val_accuracy: 0.9930\n",
      "Epoch 18/20\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.0080 - val_accuracy: 0.9990\n",
      "Epoch 19/20\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.0077 - val_accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0080 - val_accuracy: 0.9990\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2adc3184b08>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Couldn't get ovverfitted\n",
    "# Define sequential model\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Define the first layer\n",
    "model.add(keras.layers.Dense(2048, activation = 'relu', input_shape = (784,)))\n",
    "\n",
    "# Add activation function to classifier\n",
    "model.add(keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "# Finish the model compilation\n",
    "model.compile(optimizer=keras.optimizers.Adam(lr=0.01), \n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Complete the model fit operation\n",
    "model.fit(sign_language_features, sign_language_labels, epochs=20, validation_split=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_features, test_features,train_labels, test_labels = train_test_split(sign_language_features[:200],\n",
    "                                                                           sign_language_labels[:200],\n",
    "                                                                          test_size = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 784)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2adc7383d88>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_model = keras.Sequential()\n",
    "small_model.add(keras.layers.Dense(8, activation = 'sigmoid',  input_shape = (784,)))\n",
    "small_model.add(keras.layers.Dense(4, activation='softmax'))\n",
    "small_model.compile(optimizer=keras.optimizers.SGD(lr=0.01), \n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "small_model.fit(train_features, train_labels, epochs=50, verbose=0)\n",
    "\n",
    "large_model = keras.Sequential()\n",
    "large_model.add(keras.layers.Dense(64, activation = 'sigmoid',  input_shape = (784,)))\n",
    "large_model.add(keras.layers.Dense(4, activation='softmax'))\n",
    "large_model.compile(optimizer=keras.optimizers.Adam(lr=0.01), \n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "large_model.fit(train_features, train_labels, epochs=50, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 2ms/step - loss: 1.2972 - accuracy: 0.4700\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.3401 - accuracy: 0.3700\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7830 - accuracy: 0.5600\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8724 - accuracy: 0.4700\n",
      "\n",
      " Small - Train: [1.2972064018249512, 0.4699999988079071], Test: [1.3401468992233276, 0.3700000047683716]\n",
      "Large - Train: [0.7829891443252563, 0.5600000023841858], Test: [0.872368335723877, 0.4699999988079071]\n"
     ]
    }
   ],
   "source": [
    "### Again couldn't make it overfit jajaja\n",
    "# Evaluate the small model using the train data\n",
    "small_train = small_model.evaluate(train_features, train_labels)\n",
    "\n",
    "# Evaluate the small model using the test data\n",
    "small_test = small_model.evaluate(test_features, test_labels)\n",
    "\n",
    "# Evaluate the large model using the train data\n",
    "large_train = large_model.evaluate(train_features, train_labels)\n",
    "\n",
    "# Evaluate the large model using the test data\n",
    "large_test = large_model.evaluate(test_features, test_labels)\n",
    "\n",
    "# Print losses\n",
    "print('\\n Small - Train: {}, Test: {}'.format(small_train, small_test))\n",
    "print('Large - Train: {}, Test: {}'.format(large_train, large_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing to train with Estimators\n",
    "For this exercise, we'll return to the King County housing transaction dataset from chapter 2. We will again develop and train a machine learning model to predict house prices; however, this time, we'll do it using the estimator API.\n",
    "\n",
    "Rather than completing everything in one step, we'll break this procedure down into parts. We'll begin by defining the feature columns and loading the data. In the next exercise, we'll define and train a premade estimator. Note that feature_column has been imported for you from tensorflow. Additionally, numpy has been imported as np, and the Kings County housing dataset is available as a pandas DataFrame: housing.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Complete the feature column for bedrooms and add another numeric feature column for bathrooms. Use bedrooms and bathrooms as the keys.\n",
    "- Create a list of the feature columns, feature_list, in the order in which they were defined.\n",
    "- Set labels to be equal to the price column in housing.\n",
    "- Complete the bedrooms entry of the features dictionary and add another entry for bathrooms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = housing.drop('predictions', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns for bedrooms and bathrooms\n",
    "bedrooms = tf.feature_column.numeric_column(\"bedrooms\")\n",
    "bathrooms = tf.feature_column.numeric_column(\"bathrooms\")\n",
    "\n",
    "# Define the list of feature columns\n",
    "feature_list = [bedrooms, bathrooms]\n",
    "\n",
    "def input_fn():\n",
    "\t# Define the labels\n",
    "\tlabels = np.array(housing['price'])\n",
    "\t# Define the features\n",
    "\tfeatures = {'bedrooms':np.array(housing['bedrooms']), \n",
    "                'bathrooms':np.array(housing['bathrooms'])}\n",
    "\treturn features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Estimators\n",
    "In the previous exercise, you defined a list of feature columns, feature_list, and a data input function, input_fn(). In this exercise, you will build on that work by defining an estimator that makes use of input data.\n",
    "\n",
    "#### Instructions\n",
    "- Use a deep neural network regressor with 2 nodes in both the first and second hidden layers and 1 training step.\n",
    "- Modify the code to use a LinearRegressor(), remove the hidden_units, and set the number of steps to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow_estimator.python.estimator.canned.linear.LinearRegressorV2 at 0x2adc1b46fc8>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from silence_tensorflow import silence_tensorflow\n",
    "silence_tensorflow()\n",
    "# Define the model and set the number of steps\n",
    "model = tf.estimator.DNNRegressor(feature_columns=feature_list, hidden_units=[2,2])\n",
    "model.train(input_fn, steps=1)\n",
    "# Define the model and set the number of steps\n",
    "model = tf.estimator.LinearRegressor(feature_columns=feature_list)\n",
    "model.train(input_fn, steps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

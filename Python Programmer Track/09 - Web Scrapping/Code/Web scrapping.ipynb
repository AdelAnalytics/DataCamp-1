{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping in Python\n",
    "\n",
    "## Course Description\n",
    "The ability to build tools capable of retrieving and parsing information stored across the internet has been and continues to be valuable in many veins of data science. In this course, you will learn to navigate and parse html code, and build tools to crawl websites automatically. Although our scraping will be conducted using the versatile Python library scrapy, many of the techniques you learn in this course can be applied to other popular Python libraries as well, including BeautifulSoup and Selenium. Upon the completion of this course, you will have a strong mental model of html structure, will be able to build tools to parse html code and access desired information, and create a simple scrapy spiders to crawl the web at scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scrapy\n",
    "from scrapy import Selector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction to HTML\n",
    "\n",
    "Learn the structure of HTML. We begin by explaining why web scraping can be a valuable addition to your data science toolbox and then delving into some basics of HTML. We end the chapter by giving a brief introduction on XPath notation, which is used to navigate the elements within HTML code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Tree to HTML\n",
    "Here you are given the chance to create your own bit of HTML code (as a python string). More specifically, below is an HTML tree image and you will finish the missing code within the string html which produces this HTML tree.\n",
    "\n",
    "![](html_tree_exercise_resize.png)\n",
    "\n",
    "To note:\n",
    "\n",
    "- We have started the string html for you, to help nudge you in the correct direction.\n",
    "- The spacing we use in the portion of the string we provide (e.g., indenting \\<head> two spaces more than \\<html>) isn't necessary, but we did so just to make it easier to read.\n",
    "\n",
    "#### Instructions\n",
    "- Fill in the blank within the string variable html so that the HTML code matches its tree representation (including the text within the two paragraph children)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = '''\n",
    "<html>\n",
    "  <head>\n",
    "    <title>Intro HTML</title>\n",
    "  </head>\n",
    "  <body>\n",
    "    <p>Hello World!</p>\n",
    "    <p>Enjoy DataCamp!</p>\n",
    "  </body>\n",
    "</html>\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep it Classy\n",
    "In this two-part exercise, you will have a chance to show off what you've learned about attributes; in this case, we focus on the class attribute.\n",
    "\n",
    "### Instructions\n",
    "- Fill in the blank in the HTML code string html to assign a class attribute to the second div element which has the value \"you-are-classy\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whats_my_class( html ):\n",
    "  sel = Selector( text = html )\n",
    "  try:\n",
    "        print( \"The class you assigned to the second div element is:\", sel.xpath( '//div' )[1].xpath('./@class' )[0].extract() )\n",
    "  except:\n",
    "    print(\"No second div element class found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The class you assigned to the second div element is: you-are-classy\n"
     ]
    }
   ],
   "source": [
    "# HTML code string\n",
    "html = '''\n",
    "<html>\n",
    "  <body>\n",
    "    <div class=\"class1\" id=\"div1\">\n",
    "      <p class=\"class2\">Visit DataCamp!</p>\n",
    "    </div>\n",
    "    <div class = \"you-are-classy\">\n",
    "      <p class=\"class2\">Keep up the good work!</p>\n",
    "    </div>\n",
    "  </body>\n",
    "</html>\n",
    "'''\n",
    "# Print out the class of the second div element\n",
    "whats_my_class( html )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where am I?\n",
    "In this exercise, you will navigate to a specific element using your new knowledge of XPath notation.\n",
    "\n",
    "Consider the HTML code:\n",
    "\n",
    "```JavaScript\n",
    "<html>\n",
    "  <body>\n",
    "    <div>\n",
    "      <p>Good Luck!</p>\n",
    "      <p>Not here...</p>\n",
    "    </div>\n",
    "    <div>\n",
    "      <p>Where am I?</p>\n",
    "    </div>\n",
    "  </body>\n",
    "</html>\n",
    "```\n",
    "\n",
    "Your job will be to create an XPath string using single forward-slashes and brackets which navigates to the paragraph p element which contains the text \"Where am I?\".\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Using only single forward-slashes to move between generations, and brackets to select the correct element, assign a string to the variable xpath that directs to the paragraph element containing \"Where am I?\".\n",
    "- Do not include any blank spaces in the string you assign to xpath."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the blank\n",
    "xpath = '/html/body/div[2]/p'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It's Time to P\n",
    "In the lecture, we learned how to use double forward-slashes to navigate to all future generations. In this exercise, you will select all paragraph p elements within the HTML. Because we want you to navigate to all paragraph elements, it is not important that you know what the HTML code is, since the task can be accomplished with a simple XPath string using the double forward-slash notation you have learned.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Using double forward-slash notation, assign to the variable xpath a simple XPath string navigating to all paragraph p elements within any HTML code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the blank\n",
    "xpath = '//p'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A classy span\n",
    "Although we haven't yet gone deep into XPath, one thing we can do is select elements by their attributes using an XPath. For example, if we want to direct to the div element within the HTML document whose id attribute is \"uid\", then we could write the XPath string '//div[@id=\"uid\"]'. The first part of this string, //div, first looks at all div elements in the HTML document. Then, using the brackets, we specify that we want only the div element with a specific id attribute (in this case uid). To note, the phrase @id=\"uid\" in the brackets would be read as \"attribute id equals uid\".\n",
    "\n",
    "In this exercise, you will select all span elements whose class attribute equals \"span-class\". (Note: span is just another possible tag-name).\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Assign to the variable xpath an XPath string which will select all span elements whose class attribute equals \"span-class\". You do not need to see the actual HTML code to do this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the blank\n",
    "xpath = '//span[@class=\"span-class\"]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. XPaths and Selectors\n",
    "\n",
    "Leverage XPath syntax to explore scrapy selectors. Both of these concepts will move you towards being able to scrape an HTML document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Body Appendages\n",
    "We have loaded the HTML from a secret website and have used it to create a function how_many_elements(). The way this function works is that you pass it an XPath string and it will print out the number of elements the XPath you wrote has selected. For example, by running the code how_many_elements('//*') in the console will print out the total number of elements the HTML document has (try it!).\n",
    "\n",
    "Your job in this exercise is to create an XPath string which can be used to direct to all child elements the body (regardless of tag type). To note, you can first test your solution with how_many_elements() to find the total number of children in the body element if you wish.\n",
    "\n",
    "Note that the exercises in this chapter may take some time to load.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Assign to the variable xpath an XPath string which directs to all child elements of the body element. There is only one body element in this HTML document and it is a child of the root html element.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = open('data/Body_appendages.html','r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def how_many_elements( xpath ):\n",
    "    sel = Selector( text = html )\n",
    "    print( len(sel.xpath( xpath )) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "# Create an XPath string to direct to children of body element\n",
    "xpath = '//body/*'\n",
    "\n",
    "# Print out the number of elements selected\n",
    "how_many_elements( xpath )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose DataCamp!\n",
    "In this exercise, we want to give you the opportunity to create your own XPath string to achieve a certain task; the task is to select the paragraph element containing the text \"Choose DataCamp!\".\n",
    "\n",
    "Consider the following HTML:\n",
    "\n",
    "```Html\n",
    "<html>\n",
    "  <body>\n",
    "    <div>\n",
    "      <p>Hello World!</p>\n",
    "      <div>\n",
    "        <p>Choose DataCamp!</p>\n",
    "      </div>\n",
    "    </div>\n",
    "    <div>\n",
    "      <p>Thanks for Watching!</p>\n",
    "    </div>\n",
    "  </body>\n",
    "</html>\n",
    "```\n",
    "\n",
    "\n",
    "We have created the function print_element_text() for you, which will print the text contained in your element (if it contains any). Feel free to use this function to check if your solution is correct!\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Assign to the variable xpath an XPath string to direct to the paragraph element containing the phrase: \"Choose DataCamp!\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = '''\n",
    "<html>\n",
    "  <body>\n",
    "    <div>\n",
    "      <p>Hello World!</p>\n",
    "      <div>\n",
    "        <p>Choose DataCamp!</p>\n",
    "      </div>\n",
    "    </div>\n",
    "    <div>\n",
    "      <p>Thanks for Watching!</p>\n",
    "    </div>\n",
    "  </body>\n",
    "</html>\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_element_text( xpath ):\n",
    "    sel = Selector( text = html )\n",
    "    text = ' '.join( sel.xpath( xpath ).xpath( './text()' ).extract() )\n",
    "    print( text )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose DataCamp!\n"
     ]
    }
   ],
   "source": [
    "# Create an XPath string to the desired paragraph element\n",
    "xpath = '//body/div[1]/div/p'\n",
    "\n",
    "# Print out the element text\n",
    "print_element_text( xpath )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where it's @\n",
    "In this exercise, you'll begin to write an XPath string using attributes to achieve a certain task; that task is to select the paragraph element containing the text \"Thanks for Watching!\". We've already created most of the XPath string for you.\n",
    "\n",
    "Consider the following HTML:\n",
    "\n",
    "```Html\n",
    "<html>\n",
    "  <body>\n",
    "    <div id=\"div1\" class=\"class-1\">\n",
    "      <p class=\"class-1 class-2\">Hello World!</p>\n",
    "      <div id=\"div2\">\n",
    "        <p id=\"p2\" class=\"class-2\">Choose DataCamp!</p>\n",
    "      </div>\n",
    "    </div>\n",
    "    <div id=\"div3\" class=\"class-2\">\n",
    "      <p class=\"class-2\">Thanks for Watching!</p>\n",
    "    </div>\n",
    "  </body>\n",
    "</html>\n",
    "```\n",
    "\n",
    "We have created the function print_element_text() for you, which will print any text contained in your element.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Fill in the blanks in the XPath string to select the paragraph element containing the phrase: \"Thanks for Watching!\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create an Xpath string to select desired p element\n",
    "xpath = '//*[@id=\"div3\"]/p'\n",
    "\n",
    "# Print out selection text\n",
    "print_element_text( xpath )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check your Class\n",
    "This exercise is to emphasize that when you use an XPath to select an element by its class attribute without using the contains() function, you match the class exactly. Your job is to fill in the blank below and finish the variable xpath directing to the specified element.\n",
    "\n",
    "Consider the following HTML:\n",
    "\n",
    "```Html\n",
    "<html>\n",
    "  <body>\n",
    "    <div id=\"div1\" class=\"class-1\">\n",
    "      <p class=\"class-1 class-2\">Hello World!</p>\n",
    "      <div id=\"div2\">\n",
    "        <p id=\"p2\" class=\"class-2\">Choose DataCamp!</p>\n",
    "      </div>\n",
    "    </div>\n",
    "    <div id=\"div3\" class=\"class-2\">\n",
    "      <p class=\"class-2\">Thanks for Watching!</p>\n",
    "    </div>\n",
    "  </body>\n",
    "</html>\n",
    "```\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Fill in the blanks in the xpath below to select the paragraph element containing the phrase: \"Hello World!\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create an XPath string to select p element by class\n",
    "xpath = '//p[@class=\"class-1 class-2\"]'\n",
    "\n",
    "# Print out select text\n",
    "print_element_text( xpath )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper(link) Active\n",
    "One of the most important attributes to extract for \"web-crawling\" is the hyperlink url (href attribute) within an a tag. Here, you will extract such a hyperlink! We have created the function print_attribute to print out the data extracted from your XPath, so you can test your XPath strings in the console, if you like.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Fill in the blanks to complete the variable xpath below to select the href attribute value from the DataCamp hyperlink."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = '''\n",
    "<html>\n",
    "  <body>\n",
    "    <div id=\"div1\" class=\"class-1\">\n",
    "      <p class=\"class-1 class-2\">Hello World!</p>\n",
    "      <div id=\"div2\">\n",
    "        <p id=\"p2\" class=\"class-2\">Choose \n",
    "            <a href=\"http://datacamp.com\">DataCamp!</a>!\n",
    "        </p>\n",
    "      </div>\n",
    "    </div>\n",
    "    <div id=\"div3\" class=\"class-2\">\n",
    "      <p class=\"class-2\">Thanks for Watching!</p>\n",
    "    </div>\n",
    "  </body>\n",
    "</html>\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_attribute( xpath ):\n",
    "    sel = Selector(text = html)\n",
    "    print( \"You have selected:\" )\n",
    "    for i,el in enumerate(sel.xpath( xpath ).extract()):\n",
    "        print( \"%d) %s\" % (i+1, el) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have selected:\n",
      "1) http://datacamp.com\n"
     ]
    }
   ],
   "source": [
    "# Create an xpath to the href attribute\n",
    "xpath = '//p[@id=\"p2\"]/a/@href'\n",
    "\n",
    "# Print out the selection(s); there should be only one\n",
    "print_attribute( xpath )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Secret Links\n",
    "We have loaded the HTML from a secret website and have used it to create the functions how_many_elements() and preview(). The function how_many_elements() allows you to pass in an XPath string and it will print out the number of elements the XPath you wrote has selected. The function preview() allows you to pass in an XPath string and it will print out the first few elements you've selected.\n",
    "\n",
    "Your job in this exercise is to create an XPath which directs to all href attribute values of the hyperlink a elements whose class attributes contain the string \"course-block\". If you do it correctly, you should find that you have selected 40 elements with your XPath string and that it previews links (with some repetition).\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Fill in the blanks below to assign an XPath string to the variable xpath which directs to all href attribute values of the hyperlink a elements whose class attributes contain the string \"course-block\". Remember that we use the contains call within the XPath string to check if an attribute value contains a particular string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = open('data/Body_appendages.html','r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preview( xpath ):\n",
    "    sel = Selector(text = html)\n",
    "    els = sel.xpath( xpath ).extract()\n",
    "    n = len(els)\n",
    "    for i,el in enumerate( els[:min(4,n)]):\n",
    "        print( \"Element %d: %s\" % (i+1,el) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "Element 1: /courses/intro-to-python-for-data-science\n",
      "Element 2: /courses/intro-to-python-for-data-science\n",
      "Element 3: /courses/free-introduction-to-r\n",
      "Element 4: /courses/free-introduction-to-r\n"
     ]
    }
   ],
   "source": [
    "# Create an xpath to the href attributes\n",
    "xpath = '//a[contains(@class,\"course-block\")]/@href'\n",
    "\n",
    "# Print out how many elements are selected\n",
    "how_many_elements( xpath )\n",
    "# Preview the selected elements\n",
    "preview( xpath )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XPath Chaining\n",
    "Selector and SelectorList objects allow for chaining when using the xpath method. What this means is that you can apply the xpath method over once you've already applied it. For example, if sel is the name of our Selector, then\n",
    "\n",
    "`sel.xpath('/html/body/div[2]')`\n",
    "\n",
    "is the same as\n",
    "\n",
    "`sel.xpath('/html').xpath('./body/div[2]')`\n",
    "\n",
    "or is the same as\n",
    "\n",
    "`sel.xpath('/html').xpath('./body').xpath('./div[2]')`\n",
    "\n",
    "The only catch is that you need to \"glue together\" the XPath pieces by using a period at the start of each subsequent XPath string (notice the periods we added to the XPath strings in our examples).\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Fill in the blank below to chain together two xpath calls which result in the same selection as\n",
    "`sel.xpath('//div/span/p[3]')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = '\\n<html>\\n<body>\\n<div>HELLO</div>\\n<div><p>GOODBYE</p></div>\\n<div><span><p>NOPE</p><p>ALMOST</p><p>YOU GOT IT!</p></span></div>\\n</body>\\n</html>\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Selector xpath='./span/p[3]' data='<p>YOU GOT IT!</p>'>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel = Selector(text = html)\n",
    "# Chain together xpath methods to select desired p element\n",
    "sel.xpath( '//div' ).xpath( './span/p[3]' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divvy Up This Exercise\n",
    "We have pre-loaded an HTML into the string variable html. In this two part problem you will use this html variable as the HTML document to set up a Selector object with, and create a SelectorList which selects all div elements; then, you will check your understanding of what happens within the SelectorList.\n",
    "\n",
    "### Instructions \n",
    "\n",
    "- Set up the Selector object sel with the html variable passed as the text argument.\n",
    "- Assign to the variable divs a SelectorList of all div elements within the HTML document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "html =  '\\n<html>\\n<body>\\n<div>Div 1: <p>paragraph 1</p></div>\\n<div>Div 2: <p>paragraph 2</p> <p>paragraph 3</p> </div>\\n<div>Div 3: <p>paragraph 4</p> <p>paragraph 5</p> <p>paragraph 6</p></div>\\n<div>Div 4: <p>paragraph 7</p></div>\\n<div>Div 5: <p>paragraph 8</p></div>\\n</body>\\n</html>\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Selector xpath='//div' data='<div>Div 1: <p>paragraph 1</p></div>'>,\n",
       " <Selector xpath='//div' data='<div>Div 2: <p>paragraph 2</p> <p>par...'>,\n",
       " <Selector xpath='//div' data='<div>Div 3: <p>paragraph 4</p> <p>par...'>,\n",
       " <Selector xpath='//div' data='<div>Div 4: <p>paragraph 7</p></div>'>,\n",
       " <Selector xpath='//div' data='<div>Div 5: <p>paragraph 8</p></div>'>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Selector selecting html as the HTML document\n",
    "sel = Selector( text = html )\n",
    "\n",
    "# Create a SelectorList of all div elements in the HTML document\n",
    "divs = sel.xpath( '//div' )\n",
    "divs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requesting a Selector\n",
    "We have pre-loaded the URL for a particular website in the string variable url and use the requests library to put the content from the website into the string variable html. Your task is to create a Selector object sel using the HTML source code stored in html.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Fill in the two blanks below to assign to create the Selector object sel which uses the string html as the text it inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1020 elements in the HTML document.\n",
      "You have found:  1020\n"
     ]
    }
   ],
   "source": [
    "# Import requests\n",
    "import requests\n",
    "\n",
    "# Create the string html containing the HTML source\n",
    "html = requests.get( url ).content\n",
    "\n",
    "# Create the Selector object sel from html\n",
    "sel = Selector( text = html )\n",
    "\n",
    "# Print out the number of elements in the HTML document\n",
    "print( \"There are 1020 elements in the HTML document.\")\n",
    "print( \"You have found: \", len( sel.xpath('//*') ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. CSS Locators, Chaining, and Responses\n",
    "\n",
    "Learn CSS Locator syntax and begin playing with the idea of chaining together CSS Locators with XPath. We also introduce Response objects, which behave like Selectors but give us extra tools to mobilize our scraping efforts across multiple websites."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The (X)Path to CSS Locators\n",
    "Many people prefer using CSS Locator notation to XPath notation. As we will see later, it often makes attribute selection very easy. To help get you more comfortable going back and forth between XPath and CSS Locator strings, we give you a chance in this exercise to do some direct \"translation\" between the two.\n",
    "\n",
    "Note that the exercises in this chapter may take some time to load.\n",
    "\n",
    "#### Instructions \n",
    "\n",
    "- Assign to the variable css_locator a CSS Locator string which is equivalent to the XPath string given.\n",
    " - Assign to the variable xpath a XPath string which is equivalent to the CSS Locator string given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the XPath string equivalent to the CSS Locator \n",
    "xpath = '/html/body/span[1]//a'\n",
    "\n",
    "# Create the CSS Locator string equivalent to the XPath\n",
    "css_locator = 'html > body > span:nth-of-type(1) a '\n",
    "\n",
    "# Create the XPath string equivalent to the CSS Locator \n",
    "xpath = '//div[@id=\"uid\"]/span//h4'\n",
    "\n",
    "# Create the CSS Locator string equivalent to the XPath\n",
    "css_locator = 'div#uid > span h4'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get an \"a\" in this Course\n",
    "We have loaded the HTML from a secret website which you will use to set up a Selector object and the function how_many_elements(). When passing this function a CSS Locator string, it will print out the number of elements that the CSS Locator you wrote has selected.\n",
    "\n",
    "In the second part of this problem, we want you to create a CSS Locator string which will select a certain collection of elements as described here: Select the hyperlink (a element) children of all div elements belonging to the class \"course-block\" (that is, any div element with a class attribute such that \"course-block\" is one of the classes assigned). The number of such elements is 11, so you can check your solution with how_many_elements if you choose.\n",
    "\n",
    "#### Instructions \n",
    "- Fill in the blank below to create the Selector object sel using the string html as the text input.\n",
    "- Assign the variable css_locator a CSS Locator string which directs to the hyperlink (a element) children of all div elements belonging to the class \"course-block\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_html_raw(filename):\n",
    "    return open(filename,'r').read()\n",
    "\n",
    "def how_many_elements_css( css ):\n",
    "  sel = Selector( text = html )\n",
    "  print( len(sel.css( css )) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "html ='b'+ open_html_raw('data/getAnA.html')\n",
    "html1 =open_html_raw('data/getAnA.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "# Create a selector from the html (of a secret website)\n",
    "sel = Selector( text = html )\n",
    "\n",
    "# Fill in the blank\n",
    "css_locator = ' div.course-block'\n",
    "\n",
    "# Print the number of selected elements.\n",
    "how_many_elements_css( css_locator )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The CSS Wildcard\n",
    "You can use the wildcard * in CSS Locators too! In fact, we can use it in a similar way, when we want to ignore the tag type. For example:\n",
    "\n",
    "- The CSS Locator string '*' selects all elements in the HTML document.\n",
    "- The CSS Locator string '*.class-1' selects all elements which belong to class-1, but this is unnecessary since the string '.class-1' will also do the same job.\n",
    "- The CSS Locator string '*#uid' selects the element with id attribute equal to uid, but this is unnecessary since the string '#uid' will also do the same job.\n",
    "In this exercise, we want you to work by analogy with the wildcard character you know from XPath notation to discover how to select all the children of a certain element in CSS Locator notation.\n",
    "\n",
    "Instructions\n",
    "\n",
    "- Assign to the variable css_locator a CSS Locator string which will select all children (regardless of tag-type) of the unique element in the HTML document that has its id attribute equal to uid.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the CSS Locator to all children of the element whose id is uid\n",
    "css_locator = '#uid > *'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You've been 'href'ed\n",
    "In a previous exercise, you created a CSS Locator string to select the hyperlink (a element) children of all div elements belonging to the class \"course-block\". Here we have created a SelectorList called course_as having selected those hyperlink children.\n",
    "\n",
    "Now, we want you to fill in the blank below to extract the href attribute values from these elements. This is another example of chaining, as we've seen in a previous exercise.\n",
    "\n",
    "The point here is that we can chain together calls to the methods css and xpath, and combine them! We help nudge you in the correct direction by giving you the solution if we chain with another call to the css method.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Set up the Selector object sel using the string html as the text input.\n",
    "- Assign to the variable hrefs_from_xpath the href attribute values from the elements in course_as. Your solution should match hrefs_from_css!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a selector object from a secret website\n",
    "sel = Selector( text = html )\n",
    "\n",
    "# Select all hyperlinks of div elements belonging to class \"course-block\"\n",
    "course_as = sel.css( 'div.course-block > a' )\n",
    "\n",
    "# Selecting all href attributes chaining with css\n",
    "hrefs_from_css = course_as.css( '::attr(href)' )\n",
    "\n",
    "# Selecting all href attributes chaining with xpath\n",
    "hrefs_from_xpath = course_as.xpath( './@href' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Level Text\n",
    "This exercise will have you write an XPath and CSS Locator string to direct to the text of a specific paragraph p element. The p element in the HTML is uniquely defined by its id attribute, which is \"p3\". With this small piece of information, you should be able to create the desired strings; however, we have preloaded the variable html with a string containing the HTML in which this link belongs, if you want to peruse it.\n",
    "\n",
    "In this exercise, you will only be selecting the text within the element, which does not include the text in future generations of the element. We have created a function print_results for you to compare which elements your strings direct to.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Assign to the variable xpath an XPath string directing to the text within the paragraph p element with id equal to p3, which does not include the text of future generations of this p element.\n",
    "- Assign to the variable css_locator a CSS Locator string directing to this same text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "# soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = '<html><body><div id=\"this-div\"><p id=\"p1\" class=\"class-1\">This is not the element you are looking for</p><p id=\"p2\" class=\"class-12\"><a href=\"https://www.google.com\">Google</a> is linked to here, but this isn\\'t the link you are looking for. </p><p id=\"p3\" class=\"class-1 class-12\">Here is the <a href=\"https://www.datacamp.com\" id=\"a-exercise\">DataCamp</a> link you want!</p></div></body></html>'\n",
    "# print(soup.prettify())\n",
    "def our_xpath( xpath ):\n",
    "  xextr = sel.xpath( xpath ).extract()\n",
    "  return xextr\n",
    "\n",
    "def our_css( css ):\n",
    "  cextr = sel.css( css ).extract()\n",
    "  return cextr\n",
    "\n",
    "def print_results( xpath, css_locator ):\n",
    "  print( \"Your XPath extracts to following:\")\n",
    "  print( our_xpath(xpath) )\n",
    "  print(\"_________________\\n\")\n",
    "  print( \"Your CSS Locator extracts the following:\")\n",
    "  print( our_css(css_locator) )\n",
    "  return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your XPath extracts to following:\n",
      "['Here is the ', ' link you want!']\n",
      "_________________\n",
      "\n",
      "Your CSS Locator extracts the following:\n",
      "['Here is the ', ' link you want!']\n"
     ]
    }
   ],
   "source": [
    "sel = Selector(text = html)\n",
    "# Create an XPath string to the desired text.\n",
    "xpath = '//p[@id = \"p3\"]/text()  '\n",
    "\n",
    "# Create a CSS Locator string to the desired text.\n",
    "css_locator = 'p#p3::text'\n",
    "\n",
    "# Print the text from our selections\n",
    "print_results( xpath, css_locator )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Level Text\n",
    "This exercise is similar to the previous, but differs in that you will be selecting text from multiple generations of a given element.\n",
    "\n",
    "You will write an XPath and CSS Locator strings to direct to the text of a specific paragraph p element. The p element in the HTML is uniquely defined by its id attribute, which is \"p3\". With this small piece of information, you should be able to create the desired strings; however, we have preloaded the variable html with a string containing the HTML in which this link belongs, if you want to peruse it.\n",
    "\n",
    "In this exercise, you will only be selecting the text within the element which includes all text within the future generations. We have created a function print_results for you to compare which elements your strings direct to.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Assign to the variable xpath an XPath string directing to the text within the paragraph p element with id equal to p3, which includes the text of future generations of this p element.\n",
    "- Assign to the variable css_locator a CSS Locator string directing to this same text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your XPath extracts to following:\n",
      "['Here is the ', 'DataCamp', ' link you want!']\n",
      "_________________\n",
      "\n",
      "Your CSS Locator extracts the following:\n",
      "['Here is the ', 'DataCamp', ' link you want!']\n"
     ]
    }
   ],
   "source": [
    "# Create an XPath string to the desired text.\n",
    "xpath = '//p[@id = \"p3\"]//text() '\n",
    "\n",
    "# Create a CSS Locator string to the desired text.\n",
    "css_locator = 'p#p3 ::text'\n",
    "\n",
    "# Print the text from our selections\n",
    "print_results( xpath, css_locator )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reveal By Response\n",
    "We have pre-loaded a Response object, named response, with the content from a secret website. Your job is to figure out the URL and the title of the website using the response variable. You learned how to find the URL in the last lesson. To find the website title, what you need to know is:\n",
    "\n",
    "- The title is the text from the title element\n",
    "- The title element is a child of the head element, which is a child of the html root element.\n",
    "To note: the html root element only has one child head element, and the head element only has one child title element.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Assign to the variable this_url the URL used to load the response variable.\n",
    "- Assign to the variable this_title the title of the website used to load the response variable. Since we only want the text from the single element we will select, we use the extract_first() method to extract the text.\n",
    "- Regardless of whether you use xpath or css, make sure that you are selecting the text within the title element, and not just the title itself.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy.http import TextResponse\n",
    "def print_url_title( url, title ):\n",
    "  print( \"Here is what you found:\" )\n",
    "  print( \"\\t-URL: %s\" % url )\n",
    "  print( \"\\t-Title: %s\" % title )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = TextResponse('https://www.datacamp.com/courses/all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is what you found:\n",
      "\t-URL: https://www.datacamp.com/courses/all\n",
      "\t-Title: None\n"
     ]
    }
   ],
   "source": [
    "# Get the URL to the website loaded in response\n",
    "this_url = response.url\n",
    "\n",
    "# Get the title of the website loaded in response\n",
    "this_title = response.xpath('//head/title/text()').extract_first()\n",
    "\n",
    "# Print out our findings\n",
    "print_url_title( this_url, this_title )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Responding with Selectors\n",
    "Something that we should emphasize at this point about the relationship between a Selector and Response objects is that both objects return a SelectorList when using the xpath or css methods to direct to elements. In this exercise, we'll prove it to you, by having you find all hyperlink elements belonging to the class course-block__link (notice the double underscore!) and looking at the object that is produced when doing so.\n",
    "\n",
    "Recall that to find an element by class, you can use a period (.). For example, div.class-2 selects all div elements belonging to class-2.\n",
    "\n",
    "We have pre-loaded both a Response object named response and a Selector object named sel with the content from the same \"secret\" website. Once you complete the task of creating a CSS Locator, you will compare both the output from response.css and selector.css to see that they are effectively the same!\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Assign to the variable css_locator a CSS Locator string which directs to all hyperlink a elements belonging to the class course-block__link.\n",
    "- Assign to the variable response_as the output of passing the css_locator variable to the css method in response.\n",
    "- Assign to the variable sel_as the output of passing the css_locator variable to the css method in sel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = Selector(text = html1)\n",
    "\n",
    "# Create a CSS Locator string to the desired hyperlink elements\n",
    "css_locator = 'a.course-block__link'\n",
    "\n",
    "# Select the hyperlink elements from response and sel\n",
    "response_as = response.css( css_locator )\n",
    "sel_as = sel.css( css_locator )\n",
    "\n",
    "# Examine similarity\n",
    "nr = len( response_as )\n",
    "ns = len( sel_as )\n",
    "for i in range( min(nr, ns, 2) ):\n",
    "  print( \"Element %d from response: %s\" % (i+1, response_as[i]) )\n",
    "  print( \"Element %d from sel: %s\" % (i+1, sel_as[i]) )\n",
    "  print( \"\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting from a Selection\n",
    "In this exercise, you will find the text from an h4 element within a particular div element. It will occur in steps where the first step is selecting a family of div elements, and the second step is narrowing in on the first one, from which we will grab the h4 element text. This process of progressively narrowing in on elements (e.g., first to the div elements, then to the h4 element) is another example of \"chaining\", even if it doesn't look exactly the same as we've seen it before.\n",
    "\n",
    "Along the way in this exercise, there is a variable first_div set up for you to use. Think carefully about what type of object first_div is!\n",
    "\n",
    "#### Instructions \n",
    "\n",
    "- Assign to the variable divs a SelectorList which selects all div elements belonging to the class course-block.\n",
    "- Assign to the variable h4_text the text from the only h4 element within the content selected in first_div. Since we only want the text from the single element we will select, we use the extract_first() method to extract the text."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Select all desired div elements\n",
    "divs = response.css('div.course-block')\n",
    "\n",
    "# Take the first div element\n",
    "first_div = divs[0]\n",
    "\n",
    "# Extract the text from the (only) h4 element in first_div\n",
    "h4_text = first_div.css('h4::text').extract_first()\n",
    "\n",
    "# Print out the text\n",
    "print( \"The text from the h4 element is:\", h4_text )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Titular\n",
    "Similar to the work given in the previous lesson, we will have you use a pre-loaded Response object, named response to scrape the course titles from the (shortened version of the) DataCamp course directory https://www.datacamp.com/courses/all. To successfully do so, you only need to know the following\n",
    "\n",
    "The course titles are the text from all the h4 elements within the HTML document.\n",
    "We ask you to extract these course titles here.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Using response, assign to the variable crs_title_els a SelectorList of the selected course titles.\n",
    "- Assign to the variable crs_titles a list created by extracting the course titles from crs_title_els.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SelectorList of the course titles\n",
    "crs_title_els = '//h4/text()'\n",
    "\n",
    "# Extract the course titles \n",
    "crs_titles = response.xpath(crs_title_els).extract()\n",
    "\n",
    "# Print out the course titles \n",
    "for el in crs_titles:\n",
    "  print( \">>\", el )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping with Children\n",
    "We did a cute trick in the lesson to calculate how many children there were of one of the div elements belonging to the class course-block. Here we ask you to find the number of children of a mystery element (already stored within a Selector object, so you can use the xpath or css method).\n",
    "\n",
    "To be explicit, we have created the Selector object mystery in the following way:\n",
    "\n",
    "We first loaded a Response variable using a secret website as the input.\n",
    "Then we used a call to the xpath method to create a SelectorList of elements (but we won't say which ones)\n",
    "Finally, we let mystery be the first Selector object of this SelectorList.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Fill in the blank below to chain on a call to xpath so that we can calculate the number of children of the mystery element; we assign this number to the variable how_many_kids.\n",
    "\n",
    "- Remember, if you use xpath, this really is an instance of chaining, so don't forget to use a period (.) as glue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = Selector(text = html)\n",
    "mystery = sel.xpath('//body')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of elements you selected was: 1\n"
     ]
    }
   ],
   "source": [
    "# Calculate the number of children of the mystery element\n",
    "how_many_kids = len( mystery.xpath( './*' ) )\n",
    "\n",
    "# Print out the number\n",
    "print( \"The number of elements you selected was:\", how_many_kids )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Spiders\n",
    "\n",
    "Learn to create web crawlers with scrapy. These scrapy spiders will crawl the web through multiple pages, following links to scrape each of those pages automatically according to the procedures we've learned in the previous chapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy.crawler import CrawlerProcess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inheriting the Spider\n",
    "When learning about scrapy spiders, we saw that the main portion of the code for us to adjust is the class for the spider. To help build some familiarity of the class, you will complete a short piece of code to complete a toy-model of the spider class code. We've omitted the code that would actually run the spider, only including the pieces necessary to create the class.\n",
    "\n",
    "As mentioned in the lesson, a class is roughly a collection of related variables and functions housed together. Sometimes one class likes to use methods from another class, and so we will inherit methods from a different class. That's what we do in the spider class.\n",
    "\n",
    "We wrote the function inspect_class to look at the your class once you're done, if you'd like to test your solution!\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Pass scrapy.Spider as an argument to the class YourSpider; this will make it so that YourSpider inherits the methods from scrapy.Spider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_class(c):\n",
    "  newc = c()\n",
    "  meths = dir(newc)\n",
    "  if 'name' in meths:\n",
    "    print(\"Your spider class name is:\", newc.name)\n",
    "  if 'from_crawler' in meths:\n",
    "    print(\"It seems you have inherited methods from scrapy.Spider -- NICE!\")\n",
    "  else:\n",
    "    print(\"Oh no! It doesn't seem that you are inheriting the methods from scrapy.Spider!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your spider class name is: your_spider\n",
      "It seems you have inherited methods from scrapy.Spider -- NICE!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create the spider class\n",
    "class YourSpider(scrapy.Spider):\n",
    "  name = \"your_spider\"\n",
    "  # start_requests method\n",
    "  def start_requests(self):\n",
    "    pass\n",
    "  # parse method\n",
    "  def parse(self, response):\n",
    "    pass\n",
    "  \n",
    "# Inspect Your Class\n",
    "inspect_class(YourSpider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hurl the URLs\n",
    "In the next lesson we will talk about the start_requests method within the spider class. In this quick exercise, we ask you to change around a variable within the start_requests method which foreshadows some of what we will be learning in the next lesson. Basically, we want you to start becoming comfortable turning some of the wheels within a spider class; in this case, making a list of urls within the start_requests method.\n",
    "\n",
    "We've written a function inspect_class which will print out the list of elements you have in the urls variable within the start_requests method.\n",
    "\n",
    "Note: in the next several exercises, you will write code to complete your spider class, but the code does not yet include the pieces to actually run the spider; that will come at the end.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Fill in the blank within the start_requests method to assign the variable urls a list with the two strings: \"https://www.datacamp.com\" and\"https://scrapy.org\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your spider class name is: your_spider\n",
      "It seems you have inherited methods from scrapy.Spider -- NICE!\n"
     ]
    }
   ],
   "source": [
    "# Create the spider class\n",
    "class YourSpider( scrapy.Spider ):\n",
    "  name = \"your_spider\"\n",
    "  # start_requests method\n",
    "  def start_requests( self ):\n",
    "    urls = [\"https://www.datacamp.com\",\"https://scrapy.org\"]\n",
    "    for url in urls:\n",
    "      yield url\n",
    "  # parse method\n",
    "  def parse( self, response ):\n",
    "    pass\n",
    "  \n",
    "# Inspect Your Class\n",
    "inspect_class( YourSpider )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self Referencing is Classy\n",
    "You probably have noticed that within the spider class, we always input the argument self in the start_requests and parse methods (just look in the sample code in this exercise!). This allows us to reference between methods within the class. That is, if we want to refer to the method parse within the start_requests method, we would need to write self.parse rather than just parse; what writing self does is tell the code: \"Look in the same class as start_requests for a method called parse to use.\"\n",
    "\n",
    "In this exercise you will get a chance to play with this \"self referencing\".\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Fill in the required scrapy object into the class YourSpider needed to create the scrapy spider.\n",
    "- Pass the string argument \"Hello World!\" to fill in the blank in the start_requests method to use the print_msg method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square(self):\n",
    "    return self**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your spider class name is: your_spider\n",
      "It seems you have inherited methods from scrapy.Spider -- NICE!\n"
     ]
    }
   ],
   "source": [
    "# Create the spider class\n",
    "class YourSpider( scrapy.Spider ):\n",
    "  name = \"your_spider\"\n",
    "  # start_requests method\n",
    "  def start_requests( self ):\n",
    "    self.print_msg( \"Hello World!\" )\n",
    "  # parse method\n",
    "  def parse( self, response ):\n",
    "    pass\n",
    "  # print_msg method\n",
    "  def print_msg( self, msg ):\n",
    "    print( \"Calling start_requests in YourSpider prints out:\", msg )\n",
    "  \n",
    "# Inspect Your Class\n",
    "inspect_class( YourSpider )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting with Start Requests\n",
    "In the last lesson we learned about setting up the start_requests method within a scrapy spider. Here we have another toy-model spider which doesn't actually scrape anything, but gives you a chance to play with the start_requests method. What we want is for you to start becomming familiar with the arguments you pass into the scrapy.Request call within start_requests.\n",
    "\n",
    "As before, we have created the function inspect_class to examine what you are yielding in start_requests.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Fill in the required scrapy object into the class YourSpider needed to create the scrapy spider.\n",
    "- Fill in the blank in the yielded scrapy.Request call within the start_requests method so that the URL this spider would start scraping is \"https://www.datacamp.com\" and would use the parse method (within the YourSpider class) as the method to parse the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your spider class name is: your_spider\n",
      "It seems you have inherited methods from scrapy.Spider -- NICE!\n"
     ]
    }
   ],
   "source": [
    "# Create the spider class\n",
    "class YourSpider( scrapy.Spider ):\n",
    "  name = \"your_spider\"\n",
    "  # start_requests method\n",
    "  def start_requests( self ):\n",
    "    yield scrapy.Request( url = \"https://www.datacamp.com\", callback = self.parse )\n",
    "  # parse method\n",
    "  def parse( self, response ):\n",
    "    paddss\n",
    "  \n",
    "# Inspect Your Class\n",
    "inspect_class( YourSpider )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pen Names\n",
    "In this exercise, we have set up a spider class which, when finished, will retrieve the author names from a shortened version of the DataCamp course directory. The URL for the shortened version is stored in the variable url_short. Your job will be to create the list of extracted author names in the parse method of the spider.\n",
    "\n",
    "Two things you should know:\n",
    "\n",
    "You will be using the response object and the css method here.\n",
    "The course author names are defined by the text within the paragraph p elements belonging to the class course-block__author-name\n",
    "You can inspect the spider using the function inspect_spider() that we built for you -- it will print out the author names you find!\n",
    "\n",
    "Note that this and the remaining exercises in this chapter may take some time to load.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Fill in the required arguments to the parse method so that it will work as required when called in the start_requests method.\n",
    "- Within the parse method, create a variable author_names, which is a list of strings created by extracting the text from the paragraph elements belonging to the class course-block__author-name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_spider( s ):\n",
    "  news = s()\n",
    "  try:\n",
    "    req = list( news.start_requests() )[0]\n",
    "    url = req.url\n",
    "    html = requests.get( url ).content\n",
    "    response = TextResponse( url = url, body = html, encoding = 'utf-8' )\n",
    "    author_names = req.callback( response )\n",
    "    print( 'You have collected the author names:')\n",
    "    for a in author_names:\n",
    "      print('\\t-', a )\n",
    "  except:\n",
    "    print( 'Oh no! Something went wrong with the code. Keep trying!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have collected the author names:\n",
      "\t- Jonathan Cornelissen\n",
      "\t- Matt Dowle\n",
      "\t- Garrett Grolemund\n",
      "\t- Garrett Grolemund\n",
      "\t- Garrett Grolemund\n",
      "\t- Filip Schouwenaars\n",
      "\t- Gilles Inghelbrecht\n",
      "\t- Nick Carchedi\n",
      "\t- Filip Schouwenaars\n",
      "\t- Filip Schouwenaars\n",
      "\t- Mark Peterson\n"
     ]
    }
   ],
   "source": [
    "url_short = 'https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short'\n",
    "# Create the Spider class\n",
    "class DCspider( scrapy.Spider ):\n",
    "  name = 'dcspider'\n",
    "  # start_requests method\n",
    "  def start_requests( self ):\n",
    "    yield scrapy.Request( url = url_short, callback = self.parse )\n",
    "  # parse method\n",
    "  def parse( self, response ):\n",
    "    # Create an extracted list of course author names\n",
    "    author_names = response.css( 'p.course-block__author-name::text' ).extract()\n",
    "    # Here we will just return the list of Authors\n",
    "    return author_names\n",
    "  \n",
    "# Inspect the spider\n",
    "inspect_spider( DCspider )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawler Time\n",
    "This will be your first chance to play with a spider which will crawl between sites (by first collecting links from one site, and following those links to parse new sites). This spider starts at the shortened DataCamp course directory, then extracts the links of the courses in the parse method; from there, it will follow those links to extract the course descriptions from each course page in the parse_descr method, and put these descriptions into the list course_descrs. Your job is to complete the code so that the spider runs as desired!\n",
    "\n",
    "We have created a function inspect_spider which will print out one of the course descriptions you scrape (if done correctly)!\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Fill in the two blanks below (one in each of the parsing methods) with the appropriate entries so that the spider can move from the first parsing method to the second correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_spider( s ):\n",
    "  news = s()\n",
    "  try:\n",
    "    req1 = list( news.start_requests() )[0]\n",
    "    html1 = requests.get( req1.url ).content\n",
    "    response1 = TextResponse( url = req1.url, body = html1, encoding = 'utf-8' )\n",
    "    req2 = list( news.parse( response1 ) )[0]\n",
    "    html2 = requests.get( req2.url ).content\n",
    "    response2 = TextResponse( url = req2.url, body = html2, encoding = 'utf-8' )\n",
    "    for d in news.parse_descr( response2 ):\n",
    "      print(\"One course description you found is:\", d )\n",
    "      break\n",
    "  except:\n",
    "    print(\"Oh no! Something is wrong with the code. Keep trying!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One course description you found is: In this introduction to R, you will master the basics of this beautiful open source language, including factors, lists and data frames. With the knowledge gained in this course, you will be ready to undertake your first very own data analysis. With over 2 million users worldwide R is rapidly becoming the leading programming language in statistics and data science. Every year, the number of R users grows by 40% and an increasing number of organizations are using it in their day-to-day activities. Leverage the power of R by completing this free R online course today!\n"
     ]
    }
   ],
   "source": [
    "# Create the Spider class\n",
    "class DCdescr( scrapy.Spider ):\n",
    "  name = 'dcdescr'\n",
    "  # start_requests method\n",
    "  def start_requests( self ):\n",
    "    yield scrapy.Request( url = url_short, callback = self.parse )\n",
    "  \n",
    "  # First parse method\n",
    "  def parse( self, response ):\n",
    "    links = response.css( 'div.course-block > a::attr(href)' ).extract()\n",
    "    # Follow each of the extracted links\n",
    "    for link in links:\n",
    "      yield response.follow(url = link, callback = self.parse_descr)\n",
    "      \n",
    "  # Second parsing method\n",
    "  def parse_descr( self, response ):\n",
    "    # Extract course description\n",
    "    course_descr = response.css( 'p.course__description::text' ).extract_first()\n",
    "    # For now, just yield the course description\n",
    "    yield course_descr\n",
    "\n",
    "\n",
    "# Inspect the spider\n",
    "inspect_spider( DCdescr )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to Run\n",
    "In the last lesson, we went through creating an entire web-crawler to access course information from each course in the DataCamp course directory. However, the lesson seemed to stop without a climax, because we didn't play with the code after finishing the parsing methods.\n",
    "\n",
    "The point of this exercise is to remedy that!\n",
    "\n",
    "The code we give you to look at in this and the next exercise is long, because its the entire spider that took us the lesson to create! However, don't be intimidated! The point of these two exercises is to give you a very easy task to complete, with the hope that you will look at and run the code for this spider. That way, even though it is long, you will have a grasp of it!\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Fill in the one blank at the end of the parse_pages methods to assign the chapter titles to the dictionary whose key is the corresponding course title.\n",
    "- NOTE: If you hit Run Code, you must Reset to Sample Code to successfully use Run Code again!!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def previewCourses( dc_dict, n = 3 ):\n",
    "  crs_titles = list( dc_dict.keys() )\n",
    "  print( \"A preview of DataCamp Courses:\")\n",
    "  print(\"---------------------------------------\\n\")\n",
    "  for t in crs_titles[:n]:\n",
    "    print( \"TITLE: %s\" % t)\n",
    "    for i,ct in enumerate(dc_dict[t]):\n",
    "      print(\"\\tChapter %d: %s\" % (i+1,ct) )\n",
    "    print(\"\")\n",
    "url_short = 'https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-03 14:26:39 [scrapy.utils.log] INFO: Scrapy 2.1.0 started (bot: scrapybot)\n",
      "2020-05-03 14:26:39 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.9, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 20.3.0, Python 3.7.6 (default, Jan  8 2020, 20:23:39) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1  11 Sep 2018), cryptography 2.8, Platform Windows-10-10.0.18362-SP0\n",
      "2020-05-03 14:26:39 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor\n",
      "2020-05-03 14:26:39 [scrapy.crawler] INFO: Overridden settings:\n",
      "{}\n",
      "2020-05-03 14:26:39 [scrapy.extensions.telnet] INFO: Telnet Password: ccd59ad3f28cd7ca\n",
      "2020-05-03 14:26:40 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2020-05-03 14:26:41 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2020-05-03 14:26:41 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2020-05-03 14:26:41 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2020-05-03 14:26:41 [scrapy.core.engine] INFO: Spider opened\n",
      "2020-05-03 14:26:41 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2020-05-03 14:26:41 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2020-05-03 14:26:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short> (referer: None)\n",
      "2020-05-03 14:26:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/ffbc79c0169a150a45a0b503bd19662cb4d44790/free-introduction-to-r> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2020-05-03 14:26:43 [scrapy.core.engine] DEBUG: Crawled (403) <GET https://assets.datacamp.com/courses/predicting-customer-churn-in-python> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2020-05-03 14:26:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://assets.datacamp.com/courses/predicting-customer-churn-in-python>: HTTP status code is not handled or not allowed\n",
      "2020-05-03 14:26:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/844f6aa70cafd81cbc92f344baff641173604229/reporting-with-r-markdown> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2020-05-03 14:26:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/6d40286d4abe480763ff8e8ac2246c01861f8c27/intermediate-r-practice> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2020-05-03 14:26:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/5938f64803ca4a1275f63bf68ac3d300cd9f1c4a/intro-to-python-for-data-science> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2020-05-03 14:26:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/9f9f9be002e7c66df8c1733b8796943fa77b2236/introduction-to-machine-learning-with-r> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2020-05-03 14:26:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/c38792ee5ab59361c958c1b0b4453aa1385acd10/cleaning-data-in-r> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2020-05-03 14:26:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/97b54f43a3d96a7c35defb3c757ecf5471152941/intermediate-r> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2020-05-03 14:26:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/60f4cecb02a7e8e78c74643f095e3c913348da9b/dplyr-data-manipulation-r-tutorial> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2020-05-03 14:26:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/77b65bcd1c82bf793e2de583c861a077dcec2246/ggvis-data-visualization-r-tutorial> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2020-05-03 14:26:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/537eff88062cdcedb77b51d1622aa7675e2baf21/data-table-data-manipulation-r-tutorial> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2020-05-03 14:26:49 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2020-05-03 14:26:49 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 5227,\n",
      " 'downloader/request_count': 12,\n",
      " 'downloader/request_method_count/GET': 12,\n",
      " 'downloader/response_bytes': 2466147,\n",
      " 'downloader/response_count': 12,\n",
      " 'downloader/response_status_count/200': 11,\n",
      " 'downloader/response_status_count/403': 1,\n",
      " 'elapsed_time_seconds': 7.264403,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2020, 5, 3, 19, 26, 49, 221808),\n",
      " 'httperror/response_ignored_count': 1,\n",
      " 'httperror/response_ignored_status_count/403': 1,\n",
      " 'log_count/DEBUG': 12,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 12,\n",
      " 'scheduler/dequeued': 12,\n",
      " 'scheduler/dequeued/memory': 12,\n",
      " 'scheduler/enqueued': 12,\n",
      " 'scheduler/enqueued/memory': 12,\n",
      " 'start_time': datetime.datetime(2020, 5, 3, 19, 26, 41, 957405)}\n",
      "2020-05-03 14:26:49 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A preview of DataCamp Courses:\n",
      "---------------------------------------\n",
      "\n",
      "TITLE: Introduction to R\n",
      "\tChapter 1: Intro to basics\n",
      "\tChapter 2: Vectors\n",
      "\tChapter 3: Matrices\n",
      "\tChapter 4: Factors\n",
      "\tChapter 5: Data frames\n",
      "\tChapter 6: Lists\n",
      "\n",
      "TITLE: Reporting with R Markdown\n",
      "\tChapter 1: Authoring R Markdown Reports\n",
      "\tChapter 2: Embedding Code\n",
      "\tChapter 3: Compiling Reports\n",
      "\tChapter 4: Configuring R Markdown (optional)\n",
      "\n",
      "TITLE: Intermediate R - Practice\n",
      "\tChapter 1: Conditionals and Control Flow\n",
      "\tChapter 2: Loops\n",
      "\tChapter 3: Functions\n",
      "\tChapter 4: The apply family\n",
      "\tChapter 5: Utilities\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "# Import the CrawlerProcess: for running the spider\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# Create the Spider class\n",
    "class DC_Chapter_Spider(scrapy.Spider):\n",
    "  name = \"dc_chapter_spider\"\n",
    "  # start_requests method\n",
    "  def start_requests(self):\n",
    "    yield scrapy.Request(url = url_short,\n",
    "                         callback = self.parse_front)\n",
    "  # First parsing method\n",
    "  def parse_front(self, response):\n",
    "    course_blocks = response.css('div.course-block')\n",
    "    course_links = course_blocks.xpath('./a/@href')\n",
    "    links_to_follow = course_links.extract()\n",
    "    for url in links_to_follow:\n",
    "      yield response.follow(url = url,\n",
    "                            callback = self.parse_pages)\n",
    "  # Second parsing method\n",
    "  def parse_pages(self, response):\n",
    "    crs_title = response.xpath('//h1[contains(@class,\"title\")]/text()')\n",
    "    crs_title_ext = crs_title.extract_first().strip()\n",
    "    ch_titles = response.css('h4.chapter__title::text')\n",
    "    ch_titles_ext = [t.strip() for t in ch_titles.extract()]\n",
    "    dc_dict[ crs_title_ext ] = ch_titles_ext\n",
    "\n",
    "# Initialize the dictionary **outside** of the Spider class\n",
    "dc_dict = dict()\n",
    "\n",
    "# Run the Spider\n",
    "process = CrawlerProcess()\n",
    "process.crawl(DC_Chapter_Spider)\n",
    "process.start()\n",
    "\n",
    "# Print a preview of courses\n",
    "previewCourses(dc_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataCamp Descriptions\n",
    "Like the previous exercise, the code here is long since you are working with an entire web-crawling spider! But again, don't let the amount of code intimidate you, you have a handle on how spiders work now, and you are perfectly capable to complete the easy task for you here!\n",
    "\n",
    "As in the previous exercise, we have created a function previewCourses which lets you preview the output of the spider, but you can always just explore the dictionary dc_dict too after you run the code.\n",
    "\n",
    "In this exercise, you are asked to create a CSS Locator string direct to the text of the course description. All you need to know is that from the course page, the course description text is within a paragraph p element which belongs to the class course__description (two underlines).\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Fill in the one blank below in the parse_pages method with a CSS Locator string which directs to the text within the paragraph p element which belongs to the class course__description.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Import the CrawlerProcess: for running the spider\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# Create the Spider class\n",
    "class DC_Description_Spider(scrapy.Spider):\n",
    "  name = \"dc_chapter_spider\"\n",
    "  # start_requests method\n",
    "  def start_requests(self):\n",
    "    yield scrapy.Request(url = url_short,\n",
    "                         callback = self.parse_front)\n",
    "  # First parsing method\n",
    "  def parse_front(self, response):\n",
    "    course_blocks = response.css('div.course-block')\n",
    "    course_links = course_blocks.xpath('./a/@href')\n",
    "    links_to_follow = course_links.extract()\n",
    "    for url in links_to_follow:\n",
    "      yield response.follow(url = url,\n",
    "                            callback = self.parse_pages)\n",
    "  # Second parsing method\n",
    "  def parse_pages(self, response):\n",
    "    # Create a SelectorList of the course titles text\n",
    "    crs_title = response.xpath('//h1[contains(@class,\"title\")]/text()')\n",
    "    # Extract the text and strip it clean\n",
    "    crs_title_ext = crs_title.extract_first().strip()\n",
    "    # Create a SelectorList of course descriptions text\n",
    "    crs_descr = response.css( 'p.course__description::text' )\n",
    "    # Extract the text and strip it clean\n",
    "    crs_descr_ext = crs_descr.extract_first().strip()\n",
    "    # Fill in the dictionary\n",
    "    dc_dict[crs_title_ext] = crs_descr_ext\n",
    "\n",
    "# Initialize the dictionary **outside** of the Spider class\n",
    "dc_dict = dict()\n",
    "\n",
    "# Run the Spider\n",
    "process = CrawlerProcess()\n",
    "process.crawl(DC_Description_Spider)\n",
    "process.start()\n",
    "\n",
    "# Print a preview of courses\n",
    "previewCourses(dc_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capstone Crawler\n",
    "This exercise gives you a chance to show off what you've learned! In this exercise, you will write the parse function for a spider and then fill in a few blanks to finish off the spider. On the course directory page of DataCamp, each listed course has a title and a short course description. This spider will be used to scrape the course directory to extract the course titles and short course descriptions. You will not need to follow any links this time. Everything you need to know is:\n",
    "\n",
    "The course titles are defined by the text within an h4 element whose class contains the string block__title (double underline).\n",
    "The short course descriptions are defined by the text within a paragraph p element whose class contains the string block__description (double underline).\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Fill in the four blanks below with the necessary entries to complete your spider.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# parse method\n",
    "def parse(self, response):\n",
    "  # Extracted course titles\n",
    "  crs_titles = response.xpath('//h4[contains(@class,\"block__title\")]/text()').extract()\n",
    "  # Extracted course descriptions\n",
    "  crs_descrs = response.xpath('//p[contains(@class,\"block__description\")]/text()').extract()\n",
    "  # Fill in the dictionary\n",
    "  for crs_title, crs_descr in zip(crs_titles, crs_descrs):\n",
    "    dc_dict[crs_title] = crs_descr"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Create the Spider class\n",
    "class YourSpider(scrapy.Spider):\n",
    "  name = 'yourspider'\n",
    "  # start_requests method\n",
    "  def start_requests( self ):\n",
    "    yield scrapy.Request(url = url_short, callback = self.parse)\n",
    "      \n",
    "  def parse(self, response):\n",
    "    # My version of the parser you wrote in the previous part\n",
    "    crs_titles = response.xpath('//h4[contains(@class,\"block__title\")]/text()').extract()\n",
    "    crs_descrs = response.xpath('//p[contains(@class,\"block__description\")]/text()').extract()\n",
    "    for crs_title, crs_descr in zip( crs_titles, crs_descrs ):\n",
    "      dc_dict[crs_title] = crs_descr\n",
    "    \n",
    "# Initialize the dictionary **outside** of the Spider class\n",
    "dc_dict = dict()\n",
    "\n",
    "# Run the Spider\n",
    "process = CrawlerProcess()\n",
    "process.crawl(YourSpider)\n",
    "process.start()\n",
    "\n",
    "# Print a preview of courses\n",
    "previewCourses(dc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
